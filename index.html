<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Chuanguang Yang</title>
    <meta content="Chuanguang Yang, https://winycg.github.io" name="keywords">
    <style media="screen" type="text/css">
        html,
        body,
        div,
        span,
        applet,
        object,
        iframe,
        h1,
        h2,
        h3,
        h4,
        h5,
        h6,
        p,
        blockquote,
        pre,
        a,
        abbr,
        acronym,
        address,
        big,
        cite,
        code,
        del,
        dfn,
        em,
        font,
        img,
        ins,
        kbd,
        q,
        s,
        samp,
        small,
        strike,
        strong,
        sub,
        tt,
        var,
        dl,
        dt,
        dd,
        ol,
        ul,
        li,
        fieldset,
        form,
        label,
        legend,
        table,
        caption,
        tbody,
        tfoot,
        thead,
        tr,
        th,
        td {
            border: 0pt none;
            font-family: Times New Roman;
            font-size: 100%;
            font-style: inherit;
            font-weight: inherit;
            margin: 0pt;
            outline-color: invert;
            outline-style: none;
            outline-width: 0pt;
            padding: 0pt;
            vertical-align: baseline;
        }

        a {
            color: #1772d0;
            text-decoration: none;
        }

        a:focus,
        a:hover {
            color: #f09228;
            text-decoration: none;
        }

        a.paper {
            font-weight: bold;
            font-size: 12pt;
        }

        b.paper {
            font-weight: bold;
            font-size: 12pt;
        }

        * {
            margin: 0pt;
            padding: 0pt;
        }

        body {
            position: relative;
            margin: 3em auto 2em auto;
            width: 1000px;
            font-family: Times New Roman;
            font-size: 16px;
            background: #eee;
        }

        h2 {
            font-family: Times New Roman;
            font-size: 16px;
            font-weight: 700;
        }

        h3 {
            font-family: Times New Roman;
            font-size: 16px;
            font-weight: 700;
        }

        strong {
            font-family: Times New Roman;
            font-size: 16px;
            font-weight: bold;
        }

        ul {
            list-style: circle;
        }

        img {
            border: none;
        }

        li {
            padding-bottom: 0.5em;
            margin-left: 1.4em;
        }

        alert {
            font-family: Times New Roman;
            font-size: 16px;
            font-weight: bold;
            color: #FF0000;
        }

        em,
        i {
            font-style: italic;
        }

        div.section {
            clear: both;
            margin-bottom: 1.5em;
            background: #eee;
        }

        div.spanner {
            clear: both;
        }

        div.paper {
            clear: both;
            margin-top: 0.5em;
            margin-bottom: 1em;
            border: 1px solid #ddd;
            background: #fff;
            padding: 1em 1em 1em 1em;
        }

        div.paper div {
            padding-left: 230px;
        }

        img.paper {
            margin-bottom: 0.5em;
            margin-right: 0em;
            float: left;
            width: 180px;
            height: 250px;
        }

        span.blurb {
            font-style: italic;
            display: block;
            margin-top: 0.75em;
            margin-bottom: 0.5em;
        }

        pre,
        code {
            font-family: 'Lucida Console', 'Andale Mono', 'Courier', monospaced;
            margin: 1em 0;
            padding: 0;
        }

        div.paper pre {
            font-size: 0.9em;
        }

        /* navigation bar */
/* 导航栏 */
.navbar {
    display: flex; /* 使用flex布局 */
    align-items: center; /* 垂直居中 */
    background-color: #0156a3;
    position: fixed; /* fix to the top of the page. 固定在页面顶部 */
    top: 0;
    left: 0; /* 确保从最左边开始 */
    right: 0; /* 确保延伸到最右边 */
    width: 100%;
    height: 45px;
    z-index: 1000; /* ensure the navigation bar is above other content. 保证导航栏在其他内容之上 */
    padding: 0 11%; /* 添加一些内边距 */
    box-sizing: border-box; /* 确保padding包含在width内 */
}

.navbar-logo {
    display: flex;
    height: 80%;
    margin-right: 4%; /*图片和链接之间的间距*/
}

.navbar-logo img {
    height: 80%; /* 固定图片高度 */
    width: auto; /* 保持宽高比 */
}

.navbar-links {
    display: flex;
    height: 100%;
    align-items: center;
    flex-grow: 1; /* 让链接区域占据剩余空间 */
    justify-content: flex-start; /* 左对齐 */
    overflow-x: auto; /* 允许横向滚动 */
    white-space: nowrap; /* 防止换行 */
}

/* 修改链接项 */
.navbar-links a {
    display: inline-flex; /* 改为inline-flex */
    align-items: center;
    color: white;
    padding: 0 15px; /* 减少左右padding */
    height: 100%;
    text-decoration: none;
    font-size: 18px;
    scroll-margin-top: 100px;
}

/* effect on mouse hover */
/* 鼠标悬停时的效果 */
.navbar a:hover {
    background-color: #002D72;
    color: white;
    height: 100%;
}

.more-button {
    background: none;
    border: 1px solid #0156a3;
    color: #0156a3;
    padding: 5px 10px 5px 10px;
    border-radius: 4px;
    cursor: pointer;
    transition: all 0.3s;
    margin: 0px 0px 0px 0px;
    margin-left: 10px;
}

.more-button:hover {
    background: #0156a3;
    color: white;
}

.more-button i {
    margin-left: 5px;
    transition: transform 0.3s;
}

.news-list li.more-news {
    display: none;
    padding-left: 4em;
    text-indent: -4.5em;
}


    </style>

    <link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet"
          type="text/css"/>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-164510176-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-164510176-1');

      document.addEventListener('DOMContentLoaded', function() {
    document.addEventListener('click', function(e) {
        if (e.target && e.target.id === 'more-news-btn') {
            const hiddenItems = document.querySelectorAll('.more-news');
            let allVisible = true;
            
            hiddenItems.forEach(item => {
                if (item.style.display === 'none') {
                    allVisible = false;
                }
            });
            
            hiddenItems.forEach(item => {
                item.style.display = allVisible ? 'none' : 'list-item';
            });
            
            if (allVisible) {
                e.target.innerHTML = 'More News <i class="fas fa-chevron-down"></i>';
                e.target.classList.remove('active');
            } else {
                e.target.innerHTML = 'Less News <i class="fas fa-chevron-up"></i>';
                e.target.classList.add('active');
            }
        }
    });
});
    </script>

</head>


<body>

<!-- Navigation bar -->
<div class="navbar">
    <div class="navbar-logo">
        <a href="index.html"><img src="./resources/images/ict.png"></a>
    </div>
    <div class="navbar-links">
        <a href="#research_interests">Research Interests</a>
        <a href="#news">News</a>
        <a href="#Recruiting">Recruiting</a>
        <a href="#confpapers">Publications</a>
        <a href="#Honor">Honor</a>
        <a href="activity.html">Activity</a>
        <!-- <a href="/html/record.html" class="record-button" id="recordButton">Record</a> -->
    </div>
</div>

<div style="margin-bottom: 1em; border: 1px solid #ddd; background-color: #fff; padding: 1em; min-height: 180px;">
    <div style="margin: 0px auto; width: 100%;">
        
        <div style="float: left; width: 140px; text-align: center; margin-right: 20px;">
    
    <img title="implus" style="height: 140px; width: auto; display: block; margin: 0 auto;" 
         src="./resources/images/me.jpg">
    
    <div style="display: flex; justify-content: center; gap: 10px; margin-top: 10px;">
        
        <a href="https://scholar.google.com/citations?user=gYyDQqAAAAAJ" title="GoogleScholar" target="_blank">
            <img style="width: 20px; height: 20px; display: block;" src="./resources/images/google scholar.png" alt="GoogleScholar"> 
        </a>
        <a href="https://github.com/winycg" title="GitHub" target="_blank">
            <img style="width: 20px; height: 20px; display: block;" src="./resources/images/github.png" alt="GitHub"> 
        </a>

        <a href="https://blog.csdn.net/winycg" title="CSDN" target="_blank">
            <img style="width: 30px; height: 20px; display: block;" src="./resources/images/csdn.png" alt="CSDN"> 
        </a>
        <a href="https://www.xiaohongshu.com/user/profile/60589b1d00000000010034e3?xsec_token=YB7CFcWKADJJa7Xi3FchhF8DDfIPl7vl1PQV1jiY1QKoU=&xsec_source=app_share&xhsshare=&shareRedId=ODY3ODxHNU02NzUyOTgwNjY0OTc3OUs8&apptime=1766025661&share_id=d28d560403164cec936157dbd4503fa2&share_channel=wechat&wechatWid=ca1b4702ee8a85d79f6d6abe765e12a2&wechatOrigin=menu" title="RedBook" target="_blank">
            <img style="width: 20px; height: 20px; display: block;" src="./resources/images/redbook.png" alt="RedBook"> 
        </a>
        <a href="mailto:yangchuanguang@ict.ac.cn" title="Email" target="_blank">
            <img style="width: 20px; height: 20px; display: block;" src="./resources/images/mail.png" alt="Email"> 
        </a>

    </div>
    
    </div>

        <div style="padding-left: 170px; vertical-align: top;">
            <span style="line-height: 150%; font-size: 20pt;">Chuanguang Yang (杨传广)</span><br>
            <span> Institute of Computing Technology, Chinese Academy of Sciences (ICT, CAS)</span><br>
            <span><strong>Address1</strong>: No.6 Kexueyuan South Road Zhongguancun, Haidian District, Beijing, China</span><br>
            <span><strong>Address2</strong>: No.158 Beiqing Road, Haidian District, Beijing, China</span> (our main workplace, beautiful scenery and quiet environment)
            <br>
            I am an Associate Professor and Master Supervisor at the <a href="https://klais.ict.ac.cn/">State Key Laboratory of AI Safety</a>, from <a href="http://www.ict.ac.cn/">Institute of Computing Technology, Chinese Academy of Sciences</a>. 
            I got my Ph.D. degree from Institute of Computing Technology, Chinese Academy of Sciences (2018.9~2023.6), supervised by <a href="https://scholar.google.com/citations?user=daBvGcMAAAAJ">Prof. Zhulin An</a> and <a href="https://scholar.google.com/citations?user=l34KxTYAAAAJ">Prof. Yongjun Xu</a>.
            And I joined ICT, CAS in 2023.7, under the leadership of Director Yongjun Xu and Group Leader Zhulin An, focusing on Intelligent Optimization, especially Efficient AI.
        </div>
    </div>
</div>


<!--<div style="clear: both; background-color: #fff; margin-top: 1.5em; padding: .2em; padding-left: .3em;">-->


<div style="clear: both;"></div>
    <div class="section">
        <h2 id="research_interests">Research Interests
        </h2>
        <div class="paper">
            Efficient techiques (including but not limited to knowledge distillation, quantization, lightweight architecture design, pruning, cache, and dynamic inference, etc) 
            for visual recognition (e.g. image classification, object detection, semantic segmenation), 3D understanding and reconstruction (e.g. World Models, VGGT),
            AIGC models (e.g. image/video generation, multi-modal large models), and Embodied AI (VLA models).
        </div>
    </div>
</div>



<div style="clear: both;"></div>
    <div class="section">
        <h2 id="Recruiting">Recruiting
        </h2>
        <div class="paper">
            <p id="textToChange" style="white-space: pre-line;">【硕士/博士招生】课题组每年有2个硕士名额，1个博士名额。请关注计算所7月的保研夏令营暑期班、9月的保研考核和3月的考研考核 
            【实习】课题组有1~2名实习生名额，可以线上和线下，如果有兴趣请邮件联系，欢迎有志向保研的同学提前来课题组实习</p>
        </div>
    </div>
</div>


<div style="clear: both;">
    <div class="section">
        <h2 id="news">News</h2>
        <div class="paper">
            <ul>
                <li>
                    [2026.1] Three papers were accepted by ICLR-2026
                </li>
                <li>
                    [2025.12.14] CICC首届具身智能系统及应用大会“最佳具身智能技术前瞻奖”&&“最佳海报奖”
                </li>
                <li>
                    [2025.11.30] 受中国人工智能学会和中国博士后基金会邀请在2025中国人工智能大会“人工智能博士后学术交流专题”作“面向视觉理解与生成的知识蒸馏技术研究”报告，北京。[<a href="https://mp.weixin.qq.com/s/_WbIx6_0ngmJ0AgJxCOB0A">中国人工智能学会公众号</a>] [<a href="https://mp.weixin.qq.com/s/ZZC2IoBg3-vM74x5F1ivMQ">中国博士后公众号</a>]
                </li>
                <li>
                    [2025.11] I have been awarded a Research Performance Assessment Grant (First Grade) of the Postdoctoral Fellowship Program of China Postdoctoral Science Foundation. [<a href="https://mp.weixin.qq.com/s/MYxLmkL1Idj2i3CbpHStQw">中国博士后科学基金会关于印发2025年度博士后科研业绩评估考核资助结果的通知</a>]
                </li>
                <li>
                    [2025.10] I have been selected as ACM MM-2025 Outstanding Reviewer [<a href="https://dl.acm.org/action/showFmPdf?doi=10.1145%2F3746027">MM'25 frontmatter</a>]</span> 
                </li>
                <li>
                    [2025.9] One paper was accepted by NeurIPS-2025
                </li>
                <li>
                    [2025.8.4] Invited talk of "Knowledge Distillation for Visual Understanding and Generation" by 118 Club at "CCF Outstanding Doctoral Dissertation Forum"​, ​Jiaxing, Zhejiang [<a href="https://mp.weixin.qq.com/s/9cSP2bnMbhB0JX4EY8TQ_A">2025CCF优博论坛于浙江嘉兴举行</a>]
                </li>
                <li>
                    [2025.7.10] Our research group was awarded the first prize in the "Scientific and Technological Progress Award" (Technical Invention Category) of the 2025 China Institute of Command and Control Science and Technology Award. 
                </li>
                <li>
                    [2025.7] One paper was accepted by ACM MM-2025
                </li>
                <li>
                    [2025.7] One paper about Traditional Chinese Medicinal Plant Dataset was accepted by Scientific Data
                </li>
                <li>
                    [2025.6] One paper was accepted by ICCV-2025
                </li>
                <li>
                    [2025.5] One paper was accepted by SIGKDD-2025
                </li>
                <li>
                    [2025.5] One amazing review about Decision Intelligence was accepted by The Innovation
                </li>
                <li>
                    [2025.5] Two papers were accepted by ICML-2025
                </li>
                <li>
                    [2025.2] One paper was accepted by CVPR-2025
                </li>
                <li>
                    [2025.1] Three papers were accepted by AAAI-2025 (Two Oral)
                </li>
                <li class="more-news" style="display:none;">
                    [2024.7] One paper was accepted by ACM MM-2024 (Oral)
                </li>
                <li class="more-news" style="display:none;">
                    [2024.5] One paper was accepted by ICML-2024
                </li>
                <li class="more-news" style="display:none;">
                    [2024.2] One paper was accepted by CVPR-2024
                </li>
                <li class="more-news" style="display:none;">
                    [2023.12] One paper was accepted by AAAI-2024
                </li>
                <li class="more-news" style="display:none;">
                    [2023.7] One paper was accepted by ICCV-2023
                </li>
                <li class="more-news" style="display:none;">
                    [2023.3] One paper was accepted by TPAMI-2023
                </li>
                <li class="more-news" style="display:none;">
                    [2023.2] Invited review paper was accepted as the first chapter of the book《Advancements in Knowledge Distillation: Towards New Horizons of Intelligent Systems》by Springer Nature
                </li>
                <li class="more-news" style="display:none;">
                    [2022.12.7] Invited talk of "VL-Match: Enhancing Vision-Language Pretraining with Token-Level and Instance-Level Matching" at Microsoft Bing
                </li>
                <li class="more-news" style="display:none;">
                    [2022.11.15] Invited talk of "Multi-Modal Knowledge Distillation: Survey and Outlook" at MSRA
                </li>
                <li class="more-news" style="display:none;">
                    [2022.7.22] Served as a session chair of ICME-2022 Oral 36 Semantic Segmentation IV.
                </li>
                <li class="more-news" style="display:none;">
                    [2022.7] One paper was accepted by ECCV-2022
                </li>
                <li class="more-news" style="display:none;">
                    [2022.6] One paper was accepted by TNNLS-2022
                </li>
                <li class="more-news" style="display:none;">
                    [2022.4.14] Invited talk of "Knowledge Distillation: Survey and Outlook" at Gaoling School of Artificial Intelligence, Renmin University of China
                </li>
                <li class="more-news" style="display:none;">
                    [2022.3.11] Invited talk of our CVPR-2022 paper "Cross-image Relational Knowledge Distillation for Semantic Segmentation" at Horizon Robotics
                </li>
                <li class="more-news" style="display:none;">
                    [2022.3] One paper was accepted by CVPR-2022
                </li>
                <li class="more-news" style="display:none;">
                    [2022.1.21] Invited talk about "Self-supervised Contrastive Learning" at Horizon Robotics
                </li>
                <li class="more-news" style="display:none;">
                    [2021.12] Two papers were accepted by AAAI-2022
                </li>
                <li class="more-news" style="display:none;">
                    [2021.9.16] Invited talk of our IJCAI-2021 paper "Hierarchical Self-supervised Augmented Knowledge Distillation" by AI Drive and Paper weekly
                </li>
                <li class="more-news" style="display:none;">
                    [2021.4] One paper was accepted by IJCAI-2021
                </li>
                <li class="more-news" style="display:none;">
                    [2020.6.21] Invited talk of our AAAI-2020 paper "Gated Convolutional Networks with Hybrid Connectivity for Image Classification" by Student Forum on Frontiers of Artificial Intelligence (SFFAI)
                </li>
                <li class="more-news" style="display:none;">
                    [2019.11] One paper was accepted by AAAI-2020
                </li>
            </ul>
            <button id="more-news-btn" class="more-button"> More News <i class="fas fa-chevron-down"></i></button>
            <div class="spanner"></div>
        </div>
    </div>
</div>


<div style="clear: both;">
    <div class="section">
        <h2 id="confpapers">Publications [<a href="publication.html">Details</a>]</h2>
        <div class="paper">
            <ul>
                * represents corresponding author, † represents co-first author.
                <h3>2025</h3>
                <li>
                    Weilun Feng†, Haotong Qin†, <strong><font color="blue">Chuanguang Yang</font></strong>*, Xiangqi Li, Han Yang, Yuqi Li, Zhulin An*, Libo Huang, Michele Magno, Yongjun Xu.
                    <br/>
                    S²Q-VDiT: Accurate Quantized Video Diffusion Transformer with Salient Data and Sparse Token Distillation
                    <br/>
                    <i>Advances in Neural Information Processing Systems</i> <strong>(NeurIPS-2025)</strong> [<alert>CCF-A</alert>]<br>
                </li>

                <li>
                    Yanming Chen, Zixin Ma, <strong><font color="blue">Chuanguang Yang</font></strong>*, Zhulin An*, Yiwen Zhang.
                    <br/>
                    Accelerating Diffusion Models via Parallel Denoising.
                    <br/>
                    <i>ACM Multimedia</i> <strong>(ACM MM-2025)</strong> [<alert>CCF-A</alert>]<br>
                </li>

                <li>
                    Yanling Zhang, Wanhui Sun, <strong><font color="blue">Chuanguang Yang</font></strong>*, Libo Huang*, Zhulin An*, Weilun Feng, Wenjing Tang, Yongjun Xu.
                    <br/>
                    TCMP-300: A Comprehensive Traditional Chinese Medicinal Plant Dataset for Plant Recognition.
                    <br/>
                    <i>Scientific Data</i> [<alert>Nature branch journal</alert>]<br>
                </li>

                <li>
                    Yuqi Li†, <strong><font color="blue">Chuanguang Yang</font></strong>†, Hansheng Zeng, Zeyu Dong, Zhulin An, Yongjun Xu, Yingli Tian, Hao Wu*.
                    <br/>
                    Frequency-Aligned Knowledge Distillation for Lightweight Spatiotemporal Forecasting.
                    <br/>
                    <i>IEEE/CVF International Conference on Computer Vision</i> <strong>(ICCV-2025)</strong> [<alert>CCF-A</alert>]<br>
                </li>

                <li>
                    Chengqing Yu, Fei Wang*, <strong><font color="blue">Chuanguang Yang</font></strong>, Zezhi Shao, Tao Sun, Tangwen Qian, Wei Wei, Zhulin An, Yongjun Xu*.
                    <br/>
                    Merlin: Multi-View Representation Learning for Robust Multivariate Time Series Forecasting with Unfixed Missing Rates.
                    <br/>
                    <i>ACM SIGKDD Conference on Knowledge Discovery and Data Mining</i> <strong>(SIGKDD-2025)</strong> [<alert>CCF-A</alert>]<br>
                </li>
                <li>
                    Weilun Feng, <strong><font color="blue">Chuanguang Yang</font></strong>*, Haotong Qin, Xiangqi Li, Yu Wang, Zhulin An*, Libo Huang, Boyu Diao, Zixiang Zhao, Yongjun Xu, Michele Magno.
                    <br/>
                    Q-VDiT: Towards Accurate Quantization and Distillation of Video-Generation Diffusion Transformers.
                    <br/>
                    <i>International Conference on Machine Learning</i> <strong>(ICML-2025)</strong> [<alert>CCF-A</alert>]<br>
                </li>
                <li>
                    Xiangqi Li†, Libo Huang†, Zhulin An*, Weilun Feng, <strong><font color="blue">Chuanguang Yang</font></strong>, Boyu Diao, Fei Wang, Yongjun Xu.
                    <br/>
                    Geometric Feature Embedding for Effective 3D Few-Shot Class Incremental Learning.
                    <br/>
                    <i>International Conference on Machine Learning</i> <strong>(ICML-2025)</strong> [<alert>CCF-A</alert>]<br>
                </li>
                <li>
                    Han Yang, <strong><font color="blue">Chuanguang Yang</font></strong>*, Qiuli Wang, Zhulin An*, Weilun Feng, Libo Huang, Yongjun Xu.
                    <br/>
                    Multi-party Collaborative Attention Control for Image Customization.
                    <br/>
                    <i>IEEE/CVF Conference on Computer Vision and Pattern Recognition</i> <strong>(CVPR-2025)</strong> [<alert>CCF-A</alert>]<br>
                </li>
                <li>
                    Han Yang, <strong><font color="blue">Chuanguang Yang</font></strong>*, Zhulin An*, Libo Huang, Yongjun Xu.
                    <br/>
                    HSRDiff: A Hierarchical Self-Regulation Diffusion Model for Stochastic Semantic Segmentation.
                    <br/>
                    <i>AAAI Conference on Artificial Intelligence</i> <strong>(AAAI-2025)</strong> [<alert>CCF-A</alert>]<br>
                </li>
                <li>
                    Weilun Feng†, Haotong Qin†, <strong><font color="blue">Chuanguang Yang</font></strong>*, Zhulin An*, Libo Huang, Boyu Diao, Fei Wang, Renshuai Tao, Yongjun Xu, Michele Magno.
                    <br/>
                    MPQ-DM: Mixed Precision Quantization for Extremely Low Bit Diffusion Models.
                    <br/>
                    <i>AAAI Conference on Artificial Intelligence</i> <strong>(AAAI-2025)</strong> [<alert>CCF-A</alert>]<br>
                </li>
                <li>
                    <strong><font color="blue">Chuanguang Yang</font></strong>, Xinqiang Yu, Han Yang, Zhulin An*, Chengqing Yu, Libo Huang, Yongjun Xu.
                    <br/>
                    Multi-Teacher Knowledge Distillation with Reinforcement Learning for Visual Recognition.
                    <br/>
                    <i>AAAI Conference on Artificial Intelligence</i> <strong>(AAAI-2025)</strong> [<alert>CCF-A</alert>]<br>
                </li>
                <h3>2024</h3>
                <li>
                    Weilun Feng, <strong><font color="blue">Chuanguang Yang</font></strong>*, Zhulin An*, Libo Huang, Boyu Diao, Fei Wang, Yongjun Xu.
                    <br/>
                    Relational Diffusion Distillation For Efficient Image Generation.
                    <br/>
                    <i>ACM Multimedia</i> <strong>(ACM MM-2024)</strong> [<alert>CCF-A</alert>]<br>
                </li>
                <li>
                    Lujun Li, Yufan Bao, Peijie Dong, <strong><font color="blue">Chuanguang Yang</font></strong>, Anggeng Li, Wenhan Luo, Qifeng Liu, Wei Xue, Yike Guo.
                    <br/>
                    DetKDS: Knowledge Distillation Search for Object Detectors.
                    <br/>
                    <i>International Conference on Machine Learning</i> <strong>(ICML-2024)</strong> [<alert>CCF-A</alert>]<br>
                </li>
                <li>
                    Xinqiang Yu, <strong><font color="blue">Chuanguang Yang</font></strong>*, Chengqing Yu, Libo Huang*, Zhulin An*, Yongjun Xu.
                    <br/>
                    Online Policy Distillation with Decision-Attention.
                    <br/>
                    <i>International Joint Conference on Neural Networks </i> <strong>(IJCNN-2024)</strong> [<alert>CCF-C</alert>]<br>
                </li>
                <li>
                    <strong><font color="blue">Chuanguang Yang</font></strong>, Zhulin An*, Libo Huang, Junyu Bi, Xinqiang Yu, Han Yang, Boyu Diao, Yongjun Xu*.
                    <br/>
                    CLIP-KD: An Empirical Study of CLIP Model Distillation.
                    <br/>
                    <i>IEEE/CVF Conference on Computer Vision and Pattern Recognition </i> <strong>(CVPR-2024)</strong> [<alert>CCF-A</alert>]<br>
                </li>
                <li>
                    <strong><font color="blue">Chuanguang Yang</font></strong>, Luming Chen*, Erhu Zhao, Zhulin An, Yongjun Xu.
                    <br/>
                    Graph-based Representation Knowledge Distillation for Image Classification. (基于图表征知识蒸馏的图像分类方法)
                    <br/>
                    <i>Acta Electronica Sinica</i> <strong>(电子学报)</strong> [<alert>CCF-A</alert>]<br>
                </li>
                <li>
                    Libo Huang, Yan Zeng, <strong><font color="blue">Chuanguang Yang</font></strong>, Zhulin An*, Boyu Diao, Yongjun Xu.
                    <br/>
                    eTag: Class-Incremental Learning with Embedding Distillation and Task-Oriented Generation.
                    <br/>
                    <i>AAAI Conference on Artificial Intelligence</i> <strong>(AAAI-2024)</strong> [<alert>CCF-A</alert>]<br>
                </li>
                
                <h3>2023</h3>
                
                <li>
                    Junyu Bi, Daixuan Cheng, Ping Yao*, Bochen Pang, Yuefeng Zhan, <strong><font color="blue">Chuanguang Yang</font></strong> et al.
                    <br/>
                    VL-Match: Enhancing Vision-Language Pretraining with Token-Level and Instance-Level Matching.
                    <br/>
                    <i>IEEE/CVF International Conference on Computer Vision </i> <strong>(ICCV-2023)</strong> [<alert>CCF-A</alert>]<br>
                </li>
                <li>
                    <strong><font color="blue">Chuanguang Yang</font></strong>*, Xinqiang Yu, Zhulin An, Yongjun Xu.
                    <br/>
                    Categories of Response-Based, Feature-Based, and Relation-Based Knowledge Distillation.
                    <br/>
                    <i>Advancements in Knowledge Distillation: Towards New Horizons of Intelligent Systems </i> <strong>(Springer Book Chapter)</strong><br>
                </li>
                <li>
                    <strong><font color="blue">Chuanguang Yang</font></strong>, Zhulin An*, Helong Zhou, Fuzhen Zhuang, Yongjun Xu, Qian Zhang.
                    <br/>
                    Online Knowledge Distillation via Mutual Contrastive Learning for Visual Recognition.
                    <br/>
                    <i>IEEE Transactions on Pattern Analysis and Machine Intelligence </i> <strong>(TPAMI-2023)</strong> [<alert>CCF-A</alert>]<br>
                </li>
                <h3>2022</h3>
                <li>
                    <strong><font color="blue">Chuanguang Yang</font></strong>, Zhulin An*, Helong Zhou, Linhang Cai, Xiang Zhi, Jiwen Wu, Yongjun Xu, Qian Zhang.
                    <br/>
                    MixSKD: Self-Knowledge Distillation from Mixup for Image Recognition.
                    <br/>
                    <i>European Conference on Computer Vision </i> <strong>(ECCV-2022)</strong> [<alert>CCF-B</alert>]<br>
                </li>
                <li>
                    <strong><font color="blue">Chuanguang Yang</font></strong>, Zhulin An*, Linhang Cai, Yongjun Xu.
                    <br/>
                    Knowledge Distillation Using Hierarchical Self-Supervision Augmented Distribution.
                    <br/><i>IEEE Transactions on Neural Networks and Learning Systems</i>
                    <strong>(TNNLS-2022)</strong>[<alert>CCF-B, IF:14.255</alert>]<br>
                </li>
                <li>
                    <strong><font color="blue">Chuanguang Yang</font></strong>, Zhulin An*, Yongjun Xu. 
                    <br/>
                    Localizing Semantic Patches for Accelerating Image Classification.
                    <br/>
                    <i>IEEE International Conference on Multimedia and Expo</i> <strong>(ICME-2022)</strong> [<alert>CCF-B</alert>]<br>
                </li>
                <li>
                    <strong><font color="blue">Chuanguang Yang</font></strong>, Helong Zhou, Zhulin An*, Xue Jiang, Yongjun Xu, Qian Zhang.
                    <br/>
                    Cross-Image Relational Knowledge Distillation for Semantic Segmentation. 
                    <br/>
                    <i>IEEE/CVF Conference on Computer Vision and Pattern Recognition </i> <strong>(CVPR-2022)</strong> [<alert>CCF-A</alert>]<br>
                </li>
                <li>
                    <strong><font color="blue">Chuanguang Yang</font></strong>, Zhulin An*, Linhang Cai, Yongjun Xu. 
                    <br/>
                    Mutual Contrastive Learning for Visual Representation Learning.
                    <br/>
                    <i>AAAI Conference on Artificial Intelligence</i> <strong>(AAAI-2022)</strong> [<alert>CCF-A</alert>]<br>
                </li>
                <li>
                    Linhang Cai, Zhulin An*, <strong><font color="blue">Chuanguang Yang</font></strong>, Yanchun Yang, Yongjun Xu.
                    <br/>
                    Prior Gradient Mask Guided Pruning-aware Fine-tuning. <br/>
                    <i>AAAI Conference on Artificial Intelligence</i> <strong>(AAAI-2022)</strong> [<alert>CCF-A</alert>]<br>
                </li>
                <h3>2021</h3>
                <li>
                    <strong><font color="blue">Chuanguang Yang</font></strong>, Zhulin An*, Linhang Cai, Yongjun Xu. <br/>
                    Hierarchical Self-supervised Augmented Knowledge Distillation. <br/><i>International Joint Conference on Artificial Intelligence</i>
                    <strong>(IJCAI-2021)</strong> [<alert>CCF-A</alert>]<br>
                </li>
                <li>
                    <strong><font color="blue">Chuanguang Yang</font></strong>, Zhulin An*, Yongjun Xu.<br/> Multi-View Contrastive Learning for Online Knowledge Distillation.
                    <br/>
                    <i>IEEE International Conference on Acoustics, Speech and Signal Processing</i>
                    <strong>(ICASSP-2021)</strong> [<alert>CCF-B</alert>]<br>
                </li>
                <h3>2020</h3>
                <li>
                    Xiaolong Hu, Zhulin An*, <strong><font color="blue">Chuanguang Yang</font></strong>, Hui Zhu, Kaiqaing Xu, Yongjun Xu. <br>
                    DRNet: Dissect and Reconstruct the Convolutional Neural Network via Interpretable Manners. <br>
                    <i>European Conference on Artificial Intelligence</i> 
                    <strong>(ECAI-2020)</strong> [<alert>CCF-B</alert>]<br>
                </li>
                <li>
                    <strong><font color="blue">Chuanguang Yang</font></strong>, Zhulin An*, Hui Zhu, Xiaolong Hu, Kun Zhang, Kaiqiang Xu, Chao Li, Yongjun Xu.<br>
                    Gated Convolutional Networks with Hybrid Connectivity for Image Classification. <br>
                    <i>AAAI Conference on Artificial Intelligence</i> 
                    <strong>(AAAI-2020)</strong> [<alert>CCF-A</alert>]<br>
                </li>
                <h3>2019</h3>
                <li>
                    Hui Zhu, Zhulin An*, <strong><font color="blue">Chuanguang Yang</font></strong>, Kaiqiang Xu, Erhu Zhao, Yongjun Xu.<br>
                    EENA: Efficient Evolution of Neural Architecture.<br>
                    <i>International Conference on Computer Vision Workshops</i> 
                    <strong>(ICCVW-2019)</strong> [<alert>CCF-A Workshops</alert>]<br>
                </li>
                <li>
                    <strong><font color="blue">Chuanguang Yang</font></strong>, Zhulin An*, Chao Li, Boyu Diao, Yongjun Xu. <br>
                    Multi-objective Pruning for CNNs using Genetic Algorithm. <br>
                    <i>International Conference on Artificial Neural Networks</i> 
                    <strong>(ICANN-2019)</strong> [<alert>CCF-C</alert>]<br>
                </li>
                
            </ul>
            <div class="spanner"></div>
        </div>

    <div class="section">
        <br>
        <h2 id="confpapers">Published Book</h2>
        <div class="paper"><img class="paper" src="./resources/book_icon/kd_book.png"
            title="KD book">
        <div><strong>Advancements in Knowledge Distillation: Towards New Horizons of Intelligent Systems</strong><br>
            Editors: Witold Pedrycz, Shyi-Ming Chen
        <br>
        published by <i>Springer Cham</i>
        <br>
                [<a href="https://link.springer.com/book/10.1007/978-3-031-32095-8">Homepage</a>]
                <br>
                Invited by IEEE Fellow Shyi-Ming Chen, we write the first chapter (pages 1-32), titled as "Categories of Response-Based, Feature-Based, and Relation-Based Knowledge Distillation".
                In this chapter, we first survey the conventional offline KD works according to their extracted knowledge types, i.e. response, feature and relation. Benefiting the comprehensive supervision from teacher, the student could generalize better over the target task. The teacher-student-based KD has some limitations, for example, high costs for pre-training a large teacher network. Therefore, two Online KD and Self-KD schemes are proposed to improve the student without a pre-trained teacher. In practice, KD applications often face various scenarios, for example, cross-modal KD and data-free KD. Moreover, we also show some popular mechanisms to help distillation perform better, such as multi-teacher KD, attention-based KD and adversarial KD. We survey the representative works for each KD setup and summarize their main ideas and contributions. Finally, we prospect the future challenges of existing KD applications. We hope our survey can inspire future research to develop more advanced KD algorithms for improving the student performance.
        </div>
        <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./resources/book_icon/学有所承.png"
            title="学有所承">
        <div><strong>《学有所承：研究生毕业传承会撷英》</strong><br>
            本书作者来自中国科学院计算所、软件所、数学院、中国科学院大学以及北京大学、清华大学、中国人民大学、大湾区大学等院校。
        <br>
        出版社： <i>上海科学技术出版社</i>
        <br>
                [<a href="https://tuicashier.youzan.com/pay/wscgoods_order?alias=2okiikl0ti9865t&from=wsc&kdtfrom=wsc&is_info_changed=1&is_silence_auth=1">Homepage</a>]
                [<a href="https://3.cn/2oQB6u-B?jkl=@XEas14O7GMs@ MF3390">JD</a>]
                <br>
                本书是中国科学院计算技术研究所“学有所承”毕业生传承会的文萃。全书共分四篇
                <li>
                ​​第一篇“早已智珠在握”​​：辑录毕业生在科研选题、提出思路、读文献、建系统、做实验、组会交流、写论文等方面的具体建议，迹近于“术”；
                </li>
                <li>
                ​​第二篇“更悟学术道通”​​：辑录毕业生们更深层次的思考，包括方法论、心理调适、如何激发“心流”，以及“什么是格局”等思索，迹近于“道”；
                </li>
                <li>
                ​​第三篇“验谈说与后学听”​​：辑录圆桌对话环节上毕业生与在读同学们的对话，是答疑，是解惑；
                </li>
                <li>
                ​​第四篇“雏凤清于老凤声”​​：辑录研究生的导师们的所思所想，是欣慰，是期盼。
                </li>
                中国工程院院士李国杰研究员、计算所徐志伟研究员和大湾区大学副校长李晓明教授倾情作序，其意拳拳，其盼殷殷。
        </div>
        <div class="spanner"></div>
        </div>
    </div>


        <div style="clear: both;">
            <div class="section">
                <h2>Review Services</h2>
                <div class="paper">
                    <ul>
                        <li>
                            <strong>Conference</strong>: CVPR (2022~2025), ECCV (2022~2024), ICCV (2023~2025), AAAI (2022~2025), IJCAI (2023~2024),
                            NeurIPS (2024~2025), ICLR (2025), ICML (2025), ACM MM (2021~2023), ACCV (2024)
                        </li>
                        <li>
                            <strong>Journal</strong>:
                            Nature Communications, The Innovation, IEEE Transactions on Image Processing, IEEE Transactions on Neural Networks and Learning Systems, IEEE Transactions on Circuits and Systems for Video Technology, Neural Networks, 
                            Computer Vision and Image Understanding, IEEE Transactions on Big Data, Knowledge-Based Systems, Remote Sensing, The Journal of Supercomputing
                        </li>
                    </ul>
                </div>
            </div>
        </div>


        <div style="clear: both;">
            <div class="section">
                <h2 id="Honor">Honor<button class="more-button" id="HonorChinese">中文版</button><button  class="more-button" id="HonorEnglish">English</button> </h2>
                <div class="paper">
                    <ul>
                        <li>
                            <p id="poster">[2025] 1st CICC Conference on Embodied Intelligence Systems and Applications "Best Visionary Embodied Intelligence Technology Award" && "Best Poster Award"</p>
                        </li>
                        <li>
                            <p id="posdoc">[2025] Research Performance Assessment Grant (First Grade) of the Postdoctoral Fellowship Program of China Postdoctoral Science Foundation (funding: 100K yuan)</p>
                            <a href="https://mp.weixin.qq.com/s/MYxLmkL1Idj2i3CbpHStQw">中国博士后科学基金会关于印发2025年度博士后科研业绩评估考核资助结果的通知</a>
                        </li>
                        <li>
                            <p id="CICC">[2025] First prize in the "Scientific and Technological Progress Award" (Technical Invention Category) of the 2025 China Institute of Command and Control Science and Technology Award</p>
                            <a href="https://mp.weixin.qq.com/s/MToK0ALyPRPkGRwbL5KAOA">2025年度“CICC科学技术奖”评审结果公示</a>
                        </li>
                        <li>
                            <p id="Youth">[2025-2027] Youth Science Fund of the National Natural Science Foundation of China (funding: 300K yuan)</p>
                        </li>
                        <li>
                            <p id="Talents">[2024-2025] China National Postdoctoral Program for Innovative Talents (nationwide 500 selected candidates, funding: 640K yuan)</p>
                            <a href="https://www.chinapostdoctor.org.cn/article?inid=82116029-75c1-4069-b382-7363c7cd076f&catname=%E9%80%9A%E7%9F%A5%E5%85%AC%E5%91%8A">全国博士后管委会办公室关于2024年度博士后创新人才支持计划获选结果的通知</a>
                        </li>
                        <li>
                            <p id="Specific">[2023-2025] Chinese Academy of Sciences Specific Research Assistant Funding Program (funding: 240K yuan)</p>
                        </li>
                        <li>
                            <p id="ccf">[2024] CCF Doctoral Dissertation Award Nomination</p>
                            <a href="https://mp.weixin.qq.com/s/a8RQ8FCwMKoDXwjjXJ7v1g">2024年“CCF博士学位论文激励计划”评选结果公告</a>
                            
                        </li>
                        <li>
                            <p id="star">[2024] Top One Hundred New Star of ICT</p>
                        </li>
                        <li>
                            <p id="classA">[2024] Class A of Specific Assistant Researcher</p>
                        </li>
                        <li>
                            <p id="Special">[2023] Special Prize of President Scholarship, Chinese Academy of Sciences (In 2023, the ICT had only one recipient of this award, which is the highest honor granted by the Chinese Academy of Sciences to its graduate students.)</p>
                            <a href="https://youth.ucas.ac.cn/index.php/en/txkx/3697-2023">喜报！2023年度“中国科学院院长特别奖”获奖名单公布！</a>
                        </li>
                        <li>
                            <p id="Scholarship1">[2020] National Scholarship for Master's Students</p>
                        </li>
                        <li>
                            <p id="ScholarshipP">[2020] President's Scholarship of ICT</p>
                        </li>
                        <li>
                            <p id="Scholarship2">[2017] National Scholarship for Undergraduate Student</p>
                        </li>
                        <li>
                            <p id="Mathematical">[2016] First Prize of National Mathematical Contest in Modeling</p>
                        </li>
                        <li>
                            <p id="ACM">[2016] Bronze Medal of ACM-ICPC Asia Regional Contest</p>
                        </li>
                    </ul>
                    <script>
                        document.getElementById('HonorEnglish').addEventListener('click', function() {
                            document.getElementById('poster').textContent = '[2025] 1st CICC Conference on Embodied Intelligence Systems and Applications "Best Visionary Embodied Intelligence Technology Award" && "Best Poster Award"';
                            document.getElementById('posdoc').textContent = '[2025] Research Performance Assessment Grant (First Grade) of the Postdoctoral Fellowship Program of China Postdoctoral Science Foundation (funding: 100K yuan)';
                            document.getElementById('CICC').textContent = '[2025] First prize in the "Scientific and Technological Progress Award" (Technical Invention Category) of the 2025 China Institute of Command and Control Science and Technology Award';
                            document.getElementById('Youth').textContent = '[2025-2027] Youth Science Fund of the National Natural Science Foundation of China (funding: 300K yuan)';
                            document.getElementById('Talents').textContent = '[2024-2025] China National Postdoctoral Program for Innovative Talents (nationwide 500 selected candidates, funding: 640K yuan)';
                            document.getElementById('Specific').textContent = '[2023-2025] Chinese Academy of Sciences Specific Research Assistant Funding Program (funding: 240K yuan)';
                            document.getElementById('ccf').textContent = '[2024] CCF Doctoral Dissertation Award Nomination';
                            document.getElementById('star').textContent = '[2024] Top One Hundred New Star of ICT';
                            document.getElementById('classA').textContent = '[2024] Class A of Specific Assistant Researcher';
                            document.getElementById('Special').textContent = '[2023] Special Prize of President Scholarship, Chinese Academy of Sciences (In 2023, the ICT had only one recipient of this award, which is the highest honor granted by the Chinese Academy of Sciences to its graduate students.)';
                            document.getElementById('Scholarship1').textContent = '[2020] National Scholarship for Master\'s Students';
                            document.getElementById('ScholarshipP').textContent = '[2020] President\'s Scholarship of ICT';
                            document.getElementById('Scholarship2').textContent = '[2017] National Scholarship for Undergraduate Student';
                            document.getElementById('Mathematical').textContent = '[2016] First Prize of National Mathematical Contest in Modeling';
                            document.getElementById('ACM').textContent = '[2016] Bronze Medal of ACM-ICPC Asia Regional Contest';
                        });
                        document.getElementById('HonorChinese').addEventListener('click', function() {
                            document.getElementById('poster').textContent = '[2025] CICC首届具身智能系统及应用大会“最佳具身智能技术前瞻奖”&&“最佳海报奖”';
                            document.getElementById('posdoc').textContent = '[2025] 博士后科研业绩评估考核一档资助 （经费：10万元）';
                            document.getElementById('CICC').textContent = '[2025] 2025 年度中国指挥与控制学会科学技术奖“科技进步奖” （技术发明类） 一等奖';
                            document.getElementById('Youth').textContent = '[2025-2027]国家自然科学基金青年科学基金（经费：30万元）';
                            document.getElementById('Talents').textContent = '[2024-2025]博士后创新人才支持计划（全国500人，经费：64万元）';
                            document.getElementById('Specific').textContent = '[2023-2025]中国科学院特别研究助理资助项目（经费：24万元）';
                            document.getElementById('ccf').textContent = '[2024] CCF博士学位论文激励计划提名';
                            document.getElementById('star').textContent = '[2024] 中科院计算所新百星';
                            document.getElementById('classA').textContent = '[2024] 特别研究助理A类';
                            document.getElementById('Special').textContent = '[2023] 中国科学院院长特别奖（2023年度计算所仅1项，为中国科学院对在学研究生的最高奖励）';
                            document.getElementById('Scholarship1').textContent = '[2020] 硕士研究生国家奖学金';
                            document.getElementById('ScholarshipP').textContent = '[2020] 中科院计算所所长奖学金';
                            document.getElementById('Scholarship2').textContent = '[2017] 本科生国家奖学金';
                            document.getElementById('Mathematical').textContent = '[2016] 本科生全国数学建模竞赛国家一等奖';
                            document.getElementById('ACM').textContent = '[2016] ACM-ICPC亚洲区域赛铜奖';
                        });
                        </script>
                    <div class="spanner"></div>
                </div>
            </div>
        </div>

        <div style="clear: both;">
            <div class="section">
                <h2>Competitions</h2>
                <div class="paper">
                    * denotes the team leader.
                    <ul>
                        <li>
                            EPIC-SOUNDS Audio-Based Interaction Recognition Track, CVPR 2023 [<alert>Third Place Award</alert>] <br>
                            Yuqi Li, Yizhi Luo, Xiaoshuai Hao, <strong><font color="blue">Chuanguang Yang</font></strong>*, Zhulin An, Dantong Song, Wei Yi.
                        </li>
                        <li>
                            A Challenge for Out-of-Distribution Generalization in Computer Vision (OOD-CV), ICCV 2023 [<alert>Third Place Award</alert>] <br>
                            Yuqi Li, Yizhi Luo, <strong><font color="blue">Chuanguang Yang</font></strong>*, Zhulin An, Xiaoshuai Hao, Yihang Zhou.
                        </li>
                        <li>
                            Cross-Organ and Cross-Scanner Adenocarcinoma Segmentation Challenge, MICCAI 2024 [<alert>Second Place Award</alert>] <br>
                            Yuqi Li, <strong><font color="blue">Chuanguang Yang</font></strong>*, Zhulin An, Yongjun Xu, Wei Yi.
                        </li>
                    </ul>
                </div>
        
            </div>
        </div>
<!-- 临时禁用某段代码：
<div style="clear: both;">
    <div class="section">
        <h2>Research Experience </h2>
        <div class="paper">
            <ul>
                <li>
                    Microsoft Research Asia, Beijing, China. <i>Oct.2022-Jan.2023</i><br>
            
                    <ul>
                        <li style="margin-top: 0.3em">
                            Research Intern in Visual Group, mentored by <a href="https://scholar.google.com/citations?user=nZ_PVbsAAAAJ">Zheng Zhang</a>
                        </li>
                        <li>
                            Multi-Modal Knowledge Distillation, Contrastive Language-Image Pretraining
                        </li>
                    </ul>
                </li>
                
                <li>
                    Horizon Robotics, Beijing, China. <i>Aug.2021-Sep.2022</i><br>
                    <ul>
                        <li style="margin-top: 0.3em">
                            Research Intern in Basic Algorithm Platform Department, mentored by <a href="https://scholar.google.com/citations?user=pCY-bikAAAAJ">Qian Zhang</a>
                        </li>
                        <li>
                            Knowledge Distillation, Semantic Segmentation
                        </li>
                    </ul>
                </li>
            
                <li>
                    Megvii, Beijing, China. <i>Nov.2019-Apr.2020</i><br>
                    <ul>
                        <li style="margin-top: 0.3em">
                            Research Intern in Foundation Model Group, mentored by <a href="https://scholar.google.com/citations?user=yuB-cfoAAAAJ">Xiangyu Zhang</a>
                        </li>
                        <li>
                            Knowledge Distillation, Efficient Model Design
                        </li>
                    </ul>
                </li>
            
            </ul>
        </div>

    </div>
</div>
-->






</body>
</html>
