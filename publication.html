<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Chuanguang Yang</title>
    <meta content="Chuanguang Yang, https://winycg.github.io" name="keywords">
    <style media="screen" type="text/css">
        html,
        body,
        div,
        span,
        applet,
        object,
        iframe,
        h1,
        h2,
        h3,
        h4,
        h5,
        h6,
        p,
        blockquote,
        pre,
        a,
        abbr,
        acronym,
        address,
        big,
        cite,
        code,
        del,
        dfn,
        em,
        font,
        img,
        ins,
        kbd,
        q,
        s,
        samp,
        small,
        strike,
        strong,
        sub,
        tt,
        var,
        dl,
        dt,
        dd,
        ol,
        ul,
        li,
        fieldset,
        form,
        label,
        legend,
        table,
        caption,
        tbody,
        tfoot,
        thead,
        tr,
        th,
        td {
            border: 0pt none;
            font-family: Times New Roman;
            font-size: 100%;
            font-style: inherit;
            font-weight: inherit;
            margin: 0pt;
            outline-color: invert;
            outline-style: none;
            outline-width: 0pt;
            padding: 0pt;
            vertical-align: baseline;
        }

        a {
            color: #1772d0;
            text-decoration: none;
        }

        a:focus,
        a:hover {
            color: #f09228;
            text-decoration: none;
        }

        a.paper {
            font-weight: bold;
            font-size: 12pt;
        }

        b.paper {
            font-weight: bold;
            font-size: 12pt;
        }

        * {
            margin: 0pt;
            padding: 0pt;
        }

        body {
            position: relative;
            margin: 3em auto 2em auto;
            width: 1000px;
            font-family: Times New Roman;
            font-size: 16px;
            background: #eee;
        }

        h2 {
            font-family: Times New Roman;
            font-size: 16px;
            font-weight: 700;
        }

        h3 {
            font-family: Times New Roman;
            font-size: 16px;
            font-weight: 700;
        }

        strong {
            font-family: Times New Roman;
            font-size: 16px;
            font-weight: bold;
        }

        ul {
            list-style: circle;
        }

        img {
            border: none;
        }

        li {
            padding-bottom: 0.5em;
            margin-left: 1.4em;
        }

        alert {
            font-family: Times New Roman;
            font-size: 18px;
            font-weight: bold;
            color: #FF0000;
        }

        em,
        i {
            font-style: italic;
        }

        div.section {
            clear: both;
            margin-bottom: 1.5em;
            background: #eee;
        }

        div.spanner {
            clear: both;
        }

        div.paper {
            clear: both;
            margin-top: 0.5em;
            margin-bottom: 1em;
            border: 1px solid #ddd;
            background: #fff;
            padding: 1em 1em 1em 1em;
        }

        div.paper div {
            padding-left: 230px;
        }

        img.paper {
            margin-bottom: 0.5em;
            margin-right: 1em;
            float: left;
            width: 200px;
            height: 150px;
        }

        span.blurb {
            font-style: italic;
            display: block;
            margin-top: 0.75em;
            margin-bottom: 0.5em;
        }

        pre,
        code {
            font-family: Times New Roman;
            margin: 1em 0;
            padding: 0;
        }

        div.paper pre {
            font-size: 0.9em;
        }

        /* 导航栏 */
.navbar {
    display: flex; /* 使用flex布局 */
    align-items: center; /* 垂直居中 */
    background-color: #0156a3;
    position: fixed; /* fix to the top of the page. 固定在页面顶部 */
    top: 0;
    left: 0; /* 确保从最左边开始 */
    right: 0; /* 确保延伸到最右边 */
    width: 100%;
    height: 45px;
    z-index: 1000; /* ensure the navigation bar is above other content. 保证导航栏在其他内容之上 */
    padding: 0 11%; /* 添加一些内边距 */
    box-sizing: border-box; /* 确保padding包含在width内 */
}

.navbar-logo {
    display: flex;
    align-items: center;
    height: 100%;
    margin-right: 4%; /*图片和链接之间的间距*/
}
        .navbar-logo img {
    height: 80%; /* 固定图片高度 */
    width: auto; /* 保持宽高比 */
}

.navbar-logo a {
    display: inline-flex; /* 改为inline-flex */
    align-items: center;
    color: white;
    padding: 0 15px; /* 减少左右padding */
    height: 100%;
    text-decoration: none;
    font-size: 18px;
    scroll-margin-top: 100px;
}

.navbar-links {
    display: flex;
    height: 100%;
    align-items: center;
    flex-grow: 1; /* 让链接区域占据剩余空间 */
    justify-content: flex-start; /* 左对齐 */
    overflow-x: auto; /* 允许横向滚动 */
    white-space: nowrap; /* 防止换行 */
}

/* 修改链接项 */
.navbar-links a {
    display: inline-flex; /* 改为inline-flex */
    align-items: center;
    color: white;
    padding: 0 15px; /* 减少左右padding */
    height: 100%;
    text-decoration: none;
    font-size: 18px;
    scroll-margin-top: 100px;
}

/* effect on mouse hover */
/* 鼠标悬停时的效果 */
.navbar a:hover {
    background-color: #002D72;
    color: white;
    height: 100%;
}
    </style>

    <link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet"
          type="text/css"/>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-164510176-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-164510176-1');
    </script>

</head>


<body>
<!--<div style="clear: both; background-color: #fff; margin-top: 1.5em; padding: .2em; padding-left: .3em;">-->


<!-- Navigation bar -->
<div class="navbar">
    <div class="navbar-logo">
        <a href="index.html"><img src="./resources/images/ict.png"></a>
    </div>
    <div class="navbar-links">
        <a href="index.html#research_interests">Research Interests</a>
        <a href="index.html#news">News</a>
        <a href="index.html#Recruiting">Recruiting</a>
        <a href="index.html#confpapers">Publications</a>
        <a href="index.html#Honor">Honor</a>
        <!-- <a href="/html/record.html" class="record-button" id="recordButton">Record</a> -->
    </div>
</div>


<div style="clear: both;">
    <div class="section">
        <br>
        <h2 id="confpapers">Selected Publications</h2>
        <h3 id="confpapers">2025</h3>

        <div class="paper"><img class="paper" src="./resources/paper_icon/NeurIPS_2025_S2QVDiT.png"
            title="S²Q-VDiT: Accurate Quantized Video Diffusion Transformer with Salient Data and Sparse Token Distillation">
        <div><strong>S²Q-VDiT: Accurate Quantized Video Diffusion Transformer with Salient Data and Sparse Token Distillation</strong><br>
            Weilun Feng†, Haotong Qin†, <strong><font color="blue">Chuanguang Yang</font></strong>*, Xiangqi Li, Han Yang, Yuqi Li, Zhulin An*, Libo Huang, Michele Magno, Yongjun Xu.
        <br/>
        in <i>Advances in Neural Information Processing Systems</i> (NeurIPS-2025)<br>
        <alert>CCF-A, Acceptance rate: 5290/21575=24.5%</alert>
        <br>
                [<a href="https://arxiv.org/pdf/2508.04016">Paper</a>]
                [<a href="https://github.com/wlfeng0509/s2q-vdit">Code</a>]
                <br>
                We propose Hessian-aware Salient Data Selection and Attention-guided Sparse Token Distillation for accurate quantized video diffusion transformer.
        </div>
        <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./resources/paper_icon/SCI_DATA_TCMP_300.png"
            title="TCMP-300: A Comprehensive Traditional Chinese Medicinal Plant Dataset for Plant Recognition">
        <div><strong>TCMP-300: A Comprehensive Traditional Chinese Medicinal Plant Dataset for Plant Recognition</strong><br>
            Yanling Zhang, Wanhui Sun, <strong><font color="blue">Chuanguang Yang</font></strong>*, Libo Huang*, Zhulin An*, Weilun Feng, Wenjing Tang, Yongjun Xu.
        <br>
        in <i>Scientific Data-2025</i><br>
        <alert>Nature branch journal</alert>
        <br>
                [<a href="https://www.nature.com/articles/s41597-025-05522-7.pdf">Paper</a>]
                [<a href="https://github.com/winycg/TCMP-300">Code</a>]
                [<a href="./resources/citation/Scientific_data_TCMP_300.html">BibTex</a>]
                <br>
                We present a Traditional Chinese Medicinal Plant (TCMP) dataset that includes 52,089 images in 300 categories to promote the development and validation of advanced AI models for robust and accurate TCMP recognition.
        </div>
        <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./resources/paper_icon/ICCV_2025_FAKD.png"
            title="Frequency-Aligned Knowledge Distillation for Lightweight Spatiotemporal Forecasting">
        <div><strong>Frequency-Aligned Knowledge Distillation for Lightweight Spatiotemporal Forecasting</strong><br>
            Yuqi Li†, <strong><font color="blue">Chuanguang Yang</font></strong>†, Hansheng Zeng, Zeyu Dong, Zhulin An, Yongjun Xu, Yingli Tian, Hao Wu*.
        <br>
        in <i>IEEE/CVF International Conference on Computer Vision</i> (ICCV-2025)<br>
        <alert>CCF-A, Acceptance rate: 2698/11239=24%</alert>
        <br>
                [<a href="https://arxiv.org/pdf/2507.02939?">Paper</a>]
                [<a href="https://github.com/itsnotacie/ICCV25-SDKD">Code</a>]
                <br>
                We propose a frequency-aligned knowledge distillation strategy, which extracts multi-scale spectral features from the teacher's latent space, including both high and low frequency components, to guide the lightweight student model in capturing both local finegrained variations and global evolution patterns.
        </div>
        <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./resources/paper_icon/KDD_2025_Merlin.png"
            title="Merlin: Multi-View Representation Learning for Robust Multivariate Time Series Forecasting with Unfixed Missing Rates">
        <div><strong>Merlin: Multi-View Representation Learning for Robust Multivariate Time Series Forecasting with Unfixed Missing Rates</strong><br>
            Chengqing Yu, Fei Wang*, <strong><font color="blue">Chuanguang Yang</font></strong>, Zezhi Shao, Tao Sun, Tangwen Qian, Wei Wei, Zhulin An, Yongjun Xu*.
        <br>
        in <i>ACM SIGKDD Conference on Knowledge Discovery and Data Mining</i> (SIGKDD-2025)<br>
        <alert>CCF-A</alert>
        <br>
                [<a href="https://arxiv.org/pdf/2506.12459">Paper</a>]
                [<a href="https://zenodo.org/records/15508818">Code</a>]
                <br>
                We propose Multi-View Representation Learning, including offline knowledge distillation and multi-view contrastive learning, for robust multivariate time series forecasting with unfixed missing rates.
        </div>
        <div class="spanner"></div>
        </div>


        <div class="paper"><img class="paper" src="./resources/paper_icon/ICML_2025_QVDiT.png"
            title="Q-VDiT: Towards Accurate Quantization and Distillation of Video-Generation Diffusion Transformers">
        <div><strong>Q-VDiT: Towards Accurate Quantization and Distillation of Video-Generation Diffusion Transformers</strong><br>
            Weilun Feng, <strong><font color="blue">Chuanguang Yang</font></strong>*, Haotong Qin, Xiangqi Li, Yu Wang, Zhulin An*, Libo Huang, Boyu Diao, Zixiang Zhao, Yongjun Xu, Michele Magno.
        <br>
        in <i>International Conference on Machine Learning</i> (ICML-2025)<br>
        <alert>CCF-A, Acceptance rate: 3260/12107=26.9%</alert>
        <br>
                [<a href="https://openreview.net/pdf?id=Oeo53q93iP">Paper</a>]
                [<a href="https://github.com/wlfeng0509/Q-VDiT">Code</a>]
                [<a href="./resources/citation/ICML_2025_QVDiT.html">BibTex</a>]
                <br>
                We propose Token-aware Quantization Estimator and Temporal Maintenance Distillation for video DiT models.
        </div>
        <div class="spanner"></div>
        </div>

        
        <div class="paper"><img class="paper" src="./resources/paper_icon/ICML_2025_FLEG.png"
            title="Geometric Feature Embedding for Effective 3D Few-Shot Class Incremental Learning">
        <div><strong>Geometric Feature Embedding for Effective 3D Few-Shot Class Incremental Learning</strong><br>
            Xiangqi Li†, Libo Huang†, Zhulin An*, Weilun Feng, <strong><font color="blue">Chuanguang Yang</font></strong>, Boyu Diao, Fei Wang, Yongjun Xu.
        <br>
        in <i>International Conference on Machine Learning</i> (ICML-2025)<br>
        <alert>CCF-A, Acceptance rate: 3260/12107=26.9%</alert>
        <br>
                [<a href="https://openreview.net/pdf?id=CuASYs6XZW">Paper</a>]
                [<a href="https://github.com/lixiangqi707/3D-FLEG">Code</a>]
                [<a href="./resources/citation/ICML_2025_FLEG.html">BibTex</a>]
                <br>
                We propose a geometric feature embedding module by augmenting text prompts with spatial geometric features for effective 3D few-shot class incremental learning.  
        </div>
        <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./resources/paper_icon/CVPR_2025_MCA_Ctrl.png"
            title="Multi-party collaborative attention control for image customization">
        <div><strong>Multi-party Collaborative Attention Control for Image Customization</strong><br>
            Han Yang, <strong><font color="blue">Chuanguang Yang</font></strong>*, Qiuli Wang, Zhulin An*, Weilun Feng, Libo Huang, Yongjun Xu.
        <br>
        in <i>IEEE/CVF Conference on Computer Vision and Pattern Recognition</i> (CVPR-2025)<br>
        <alert>CCF-A, Acceptance rate: 2878/13008=22.1%</alert>
        <br>
                [<a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Yang_Multi-party_Collaborative_Attention_Control_for_Image_Customization_CVPR_2025_paper.pdf">Paper</a>]
                [<a href="https://github.com/yanghan-yh/MCA-Ctrl">Code</a>]
                [<a href="./resources/citation/CVPR_2025_MCA_Ctrl.html">BibTex</a>]
                <br>
                We propose Multi-party Collaborative Attention Control (MCA-Ctrl), a tuning-free method that enables high-quality image customization using both text and complex visual conditions. 
        </div>
        <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./resources/paper_icon/AAAI_2025_MTKD_RL.png"
            title="Multi-Teacher Knowledge Distillation with Reinforcement Learning for Visual Recognition">
        <div><strong>Multi-Teacher Knowledge Distillation with Reinforcement Learning for Visual Recognition</strong><br>
            <strong><font color="blue">Chuanguang Yang</font></strong>, Xinqiang Yu, Han Yang, Zhulin An*, Chengqing Yu, Libo Huang, Yongjun Xu.
        <br>
        in <i>AAAI Conference on Artificial Intelligence</i> (AAAI-2025 Oral)<br>
        <alert>CCF-A, Acceptance rate: 3032/12957=23.40%</alert><br>
        <alert>Oral rate: 600/12957=4.63%</alert>
        <br>
                [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/32990/35145">Paper</a>]
                [<a href="https://github.com/winycg/MTKD-RL">Code</a>]
                [<a href="./resources/citation/AAAI_2025_MTKD_RL.html">BibTex</a>]
                <br>
                We propose Multi-Teacher Knowledge Distillation with Reinforcement Learning (MTKD-RL) to dynamically optimize multi-teacher weights for visual recognition models. 
        </div>
        <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./resources/paper_icon/AAAI_2025_MPQ_DM.png"
            title="MPQ-DM: Mixed Precision Quantization for Extremely Low Bit Diffusion Models">
        <div><strong>MPQ-DM: Mixed Precision Quantization for Extremely Low Bit Diffusion Models</strong><br>
            Weilun Feng†, Haotong Qin†, <strong><font color="blue">Chuanguang Yang</font></strong>*, Zhulin An*, Libo Huang, Boyu Diao, Fei Wang, Renshuai Tao, Yongjun Xu, Michele Magno.
        <br>
        in <i>AAAI Conference on Artificial Intelligence</i> (AAAI-2025 Oral)<br>
        <alert>CCF-A, Acceptance rate: 3032/12957=23.40%</alert><br>
        <alert>Oral rate: 600/12957=4.63%</alert>
        <br>
                [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/33823/35978">Paper</a>]
                [<a href="https://github.com/cantbebetter2/MPQ-DM">Code</a>]
                [<a href="./resources/citation/AAAI_2025_MPQ_DM.html">BibTex</a>]
                <br>
                We propose MPQ-DM, a Mixed-Precision Quantization method for extremely low bit diffusion models. 
        </div>
        <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./resources/paper_icon/AAAI_2025_HSRDiff.png"
            title="HSRDiff: A Hierarchical Self-Regulation Diffusion Model for Stochastic Semantic Segmentation">
        <div><strong>HSRDiff: A Hierarchical Self-Regulation Diffusion Model for Stochastic Semantic Segmentation</strong><br>
            Han Yang, <strong><font color="blue">Chuanguang Yang</font></strong>*, Zhulin An*, Libo Huang, Yongjun Xu.
        <br>
        in <i>AAAI Conference on Artificial Intelligence</i> (AAAI-2025)<br>
        <alert>CCF-A, Acceptance rate: 3032/12957=23.40%</alert>
        <br>
                [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/32163/34318">Paper</a>]
                [<a href="https://github.com/yanghan-yh/HSRDiff">Code</a>]
                [<a href="./resources/citation/AAAI_2025_HSRDiff.html">BibTex</a>]
                <br>
                We propose a Hierarchical Self-Regulation Diffusion (HSRDiff) Model for stochastic semantic segmentation.
        </div>
        <div class="spanner"></div>
        </div>


        <h3 id="confpapers">2024</h3>
        
        <div class="paper"><img class="paper" src="./resources/paper_icon/ACMMM_2024_RDD.jpg"
            title="Relational Diffusion Distillation for Efficient Image Generation">
        <div><strong>Relational Diffusion Distillation for Efficient Image Generation</strong><br>
            Weilun Feng, <strong><font color="blue">Chuanguang Yang</font></strong>, Zhulin An, Libo Huang, Boyu Diao, Fei Wang, Yongjun Xu.
        <br>
        in <i>ACM International Conference on Multimedia</i> (ACM MM-2024 Oral)<br>
        <alert>CCF-A, Acceptance rate: 1149/4385=26.20%</alert><br>
        <alert>Oral rate: 174/4385=3.97%</alert>
        <br>
                [<a href="https://dl.acm.org/doi/pdf/10.1145/3664647.3680768">Paper</a>]
                [<a href="https://github.com/cantbebetter2/RDD">Code</a>]
                [<a href="./resources/citation/ACMMM_2024_RDD.html">BibTex</a>]
                <br>
                We propose Relational Diffusion Distillation, a novel distillation method tailored specifically for distilling diffusion models.
        </div>
        <div class="spanner"></div>
        </div>


        <div class="paper"><img class="paper" src="./resources/paper_icon/ICML_2024_DetKDS.jpg"
            title="DetKDS: Knowledge Distillation Search for Object Detectors">
        <div><strong>DetKDS: Knowledge Distillation Search for Object Detectors</strong><br>
            Lujun Li, Yufan Bao, Peijie Dong, <strong><font color="blue">Chuanguang Yang</font></strong>, Anggeng Li, Wenhan Luo, Qifeng Liu, Wei Xue, Yike Guo.
        <br>
        in <i>International Conference on Machine Learning</i> (ICML-2024)<br>
        <alert>CCF-A, Acceptance rate: 2610/9473=27.5%</alert>
        <br>
                [<a href="https://raw.githubusercontent.com/mlresearch/v235/main/assets/li24c/li24c.pdf">Paper</a>]
                [<a href="https://github.com/lliai/DetKDS">Code</a>]
                [<a href="./resources/citation/ICML_2024_DetKDS.html">BibTex</a>]
                <br>
                We leverage search algorithms to discover optimal distillers for object detectors.
        </div>
        <div class="spanner"></div>
        </div>


        <div class="paper"><img class="paper" src="./resources/paper_icon/IJCNN_2024_OPD.jpg"
            title="Online Policy Distillation with Decision-Attention">
        <div><strong>Online Policy Distillation with Decision-Attention</strong><br>
            Xinqiang Yu, <strong><font color="blue">Chuanguang Yang</font></strong>, Chengqing Yu, Libo Huang, Zhulin An, Yongjun Xu.
        <br>
        in <i>International Joint Conference on Neural Networks</i> (IJCNN-2024)<br>
        <alert>CCF-C, Acceptance rate: 1701/3272=51.99%</alert>
        <br>
                [<a href="https://arxiv.org/pdf/2406.05488">Paper</a>]
                [<a href="./resources/citation/IJCNN_2024_OPD.html">BibTex</a>]
                <br>
                We propose Online Policy Distillation with Decision-Attention to make various policies operate in the same environment learn different perspectives.
        </div>
        <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./resources/paper_icon/CVPR_2024_CLIP_KD.jpg"
            title="CLIP-KD: An Empirical Study of CLIP Model Distillation">
        <div><strong>CLIP-KD: An Empirical Study of CLIP Model Distillation</strong><br>
            <strong><font color="blue">Chuanguang Yang</font></strong>, Zhulin An, Libo Huang, Junyu Bi, Xinqiang Yu, Han Yang, Boyu Diao, Yongjun Xu.
        <br>
        in <i>IEEE/CVF Conference on Computer Vision and Pattern Recognition</i> (CVPR-2024)<br>
        <alert>CCF-A, Acceptance rate: 2719/11532=23.6%</alert>
        <br>
                [<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_CLIP-KD_An_Empirical_Study_of_CLIP_Model_Distillation_CVPR_2024_paper.pdf">Paper</a>]
                [<a href="https://github.com/winycg/CLIP-KD">Code</a>]
                [<a href="./resources/citation/CVPR_2024_CLIP_KD.html">BibTex</a>]
                <br>
                We propose several distillation strategies, including relation, feature, gradient and contrastive paradigms, to examine the effectiveness of CLIP-Knowledge Distillation.
        </div>
        <div class="spanner"></div>
        </div>
        
        <div class="paper"><img class="paper" src="./resources/paper_icon/AAAI_2024_eTag.jpg"
            title="eTag: Class-Incremental Learning with Embedding Distillation and Task-Oriented Generation">
        <div><strong>eTag: Class-Incremental Learning with Embedding Distillation and Task-Oriented Generation</strong><br>
        Libo Huang, Yan Zeng, <strong><font color="blue">Chuanguang Yang</font></strong>, Zhulin An, Boyu Diao, Yongjun Xu.<br>
        in <i>AAAI Conference on Artificial Intelligence</i> (AAAI-2024)<br>
        <alert>CCF-A, Acceptance rate: 2342/12100=23.8%</alert>
        <br>
                [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/29153/30181">Paper</a>]
                [<a href="https://github.com/libo-huang/eTag">Code</a>]
                [<a href="./resources/citation/AAAI_2024_eTag.html">BibTex</a>]
                <br>
                We propose a method of embedding distillation and task-oriented generation for class-incremental learning, which requires
neither the exemplar nor the prototype.
        </div>
        <div class="spanner"></div>
        </div>

        <h3 id="confpapers">2023</h3>
        <div class="paper"><img class="paper" src="./resources/paper_icon/ICCV_2023_VL_MATCH.jpg"
            title="VL-Match: Enhancing Vision-Language Pretraining with Token-Level and Instance-Level Matching">
        <div><strong>VL-Match: Enhancing Vision-Language Pretraining with Token-Level and Instance-Level Matching</strong><br>
        Junyu Bi, Daixuan Cheng, Ping Yao, Bochen Pang, Yuefeng Zhan, <strong><font color="blue">Chuanguang Yang</font></strong>, et al.<br>
        in <i>IEEE/CVF International Conference on Computer Vision</i> (ICCV-2023)<br>
        <alert>CCF-A, Acceptance rate: 2160/8068=26.8%</alert>
        <br>
                [<a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Bi_VL-Match_Enhancing_Vision-Language_Pretraining_with_Token-Level_and_Instance-Level_Matching_ICCV_2023_paper.pdf">Paper</a>]
                [<a href="./resources/citation/ICCV_2023_VL_Match.html">BibTex</a>]
                <br>
                We
propose VL-Match, a Vision-Language framework with Enhanced Token-level and Instance-level Matching. 
        </div>
        <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./resources/paper_icon/Springer_2023_survey.jpg"
            title="Categories of Response-Based, Feature-Based, and Relation-Based Knowledge Distillation">
        <div><strong>Categories of Response-Based, Feature-Based, and Relation-Based Knowledge Distillation</strong><br>
        <strong><font color="blue">Chuanguang Yang</font></strong>, Xinqiang Yu, Zhulin An, Yongjun Xu.<br>
        in <i>Advancements in Knowledge Distillation: Towards New Horizons of Intelligent Systems</i> (Springer Book Chapter)<br>
        <alert>Invited Survey Paper</alert>
        <br>
                [<a href="https://arxiv.org/pdf/2306.10687.pdf">Paper</a>]
                [<a href="./resources/citation/springer_2023_kd_survey.html">BibTex</a>]
                <br>
                This paper provides a comprehensive KD survey,
including knowledge categories, distillation schemes and algorithms, as
well as some empirical studies on performance comparison. 
        </div>
        <div class="spanner"></div>
        </div>


        <div class="paper"><img class="paper" src="./resources/paper_icon/TPAMI_2023_L_MCL.jpg"
            title="Online Knowledge Distillation via Mutual Contrastive Learning for Visual Recognition.">
        <div><strong>Online Knowledge Distillation via Mutual Contrastive Learning for Visual Recognition</strong><br>
        <strong><font color="blue">Chuanguang Yang</font></strong>, Zhulin An, Helong Zhou, Fuzhen Zhuang, Yongjun Xu, Qian Zhang.<br>
        in <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i> (TPAMI-2023)<br>
        <alert>CCF-A</alert>
        <br>
                [<a href="https://arxiv.org/pdf/2207.11518.pdf">Paper</a>]
                [<a href="https://github.com/winycg/L-MCL">Code</a>]
                [<a href="./resources/citation/TPAMI_2023_LMCL.html">BibTex</a>]
                <br>
                We present a Mutual Contrastive Learning (MCL) framework for online KD. The core idea of MCL is to perform mutual interaction and transfer of contrastive distributions
among a cohort of networks in an online manner. 
        </div>
        <div class="spanner"></div>
        </div>

        <h3 id="confpapers">2022</h3>
        <div class="paper"><img class="paper" src="./resources/paper_icon/ECCV_2022_MixSKD.jpg"
            title="MixSKD: Self-Knowledge Distillation from Mixup for Image Recognition.">
        <div><strong>MixSKD: Self-Knowledge Distillation from Mixup for Image Recognition</strong><br>
        <strong><font color="blue">Chuanguang Yang</font></strong>, Zhulin An, Helong Zhou, Linhang Cai, Xiang Zhi, Jiwen Wu, Yongjun Xu, Qian Zhang.<br>
        in <i>European Conference on Computer Vision</i> (ECCV-2022)<br>
        <alert>CCF-B, Acceptance rate: 1650/5803=28.4%</alert>
        <br>
                [<a href="https://arxiv.org/pdf/2208.05768.pdf">Paper</a>]
                [<a href="https://github.com/winycg/Self-KD-Lib">Code</a>]
                [<a href="./resources/citation/ECCV_2022_MixSKD.html">BibTex</a>]
                <br>
                This paper presents MixSKD, a powerful Self-KD method to regularize the network to behave linearly in feature maps and class probabilities between samples
                using Mixup images.
        </div>
        <div class="spanner"></div>
        </div>


        <div class="paper"><img class="paper" src="./resources/paper_icon/TNNLS_HSSAKD.png"
            title="Knowledge Distillation Using Hierarchical Self-Supervision Augmented Distribution">
        <div><strong>Knowledge Distillation Using Hierarchical Self-Supervision Augmented Distribution</strong><br>
        <strong><font color="blue">Chuanguang Yang</font></strong>, Zhulin An, Linhang Cai, Yongjun Xu<br>
        in <i>IEEE Transactions on Neural Networks and Learning Systems</i> (TNNLS-2022)<br>
        <alert>CCF-B, IF:14.255</alert>
        <br>
                [<a href="https://arxiv.org/pdf/2109.03075.pdf">Paper</a>]
                [<a href="https://github.com/winycg/HSAKD">Code</a>]
                [<a href="./resources/citation/TNNLS_2022_HSSAKD.html">BibTex</a>]
                <br>
        We investigate an auxiliary self-supervision augmented distributionan for training a single network, offline KD and online KD.
        </div>
        <div class="spanner"></div>
        </div>


        <div class="paper"><img class="paper" src="./resources/paper_icon/ICME_2022_anchornet.png"
            title="Localizing Semantic Patches for Accelerating Image Classification">
        <div><strong>Localizing Semantic Patches for Accelerating Image Classification</strong><br>
        <strong><font color="blue">Chuanguang Yang</font></strong>, Zhulin An, Yongjun Xu<br>
        in <i>IEEE International Conference on Multimedia and Expo</i> (ICME-2022)<br>
        <alert>CCF-B, Acceptance rate: 381/1285=29.6%</alert>
        <br>
                [<a href="https://arxiv.org/pdf/2206.03367.pdf">Paper</a>]
                [<a href="https://github.com/winycg/AnchorNet">Code</a>]
                [<a href="./resources/citation/ICME_2022_Localizing.html">BibTex</a>]
                <br>
        We propose an interpretable AnchorNet to localize semantic patches for accelerating image classification.
        </div>
        <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./resources/paper_icon/CVPR_2022_CIRKD.png"
            title="Cross-Image Relational Knowledge Distillation for Semantic Segmentation.">
        <div><strong>Cross-Image Relational Knowledge Distillation for Semantic Segmentation</strong><br>
            <strong><font color="blue">Chuanguang Yang</font></strong>, Helong Zhou, Zhulin An, Xue Jiang, Yongjun Xu, Qian Zhang<br>
        in <i>IEEE/CVF Conference on Computer Vision and Pattern Recognition </i> (CVPR-2022)<br>
        <alert>CCF-A, Acceptance rate: 2067/8161=25.3%</alert>
        <br>
                [<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Cross-Image_Relational_Knowledge_Distillation_for_Semantic_Segmentation_CVPR_2022_paper.pdf">Paper</a>]
                [<a href="https://github.com/winycg/CIRKD">Code</a>]
                [<a href="./resources/citation/CVPR_2022_CIRKD.html">BibTex</a>]
                <br>
         We propose cross-Image relational knowledge distillation for semantic segmentation.
        
        </div>
        <div class="spanner"></div>
        </div>


        <div class="paper"><img class="paper" src="./resources/paper_icon/AAAI_2022_MCL.png"
                                title="Mutual Contrastive Learning for Visual Representation Learning">
            <div><strong>Mutual Contrastive Learning for Visual Representation Learning</strong><br>
                <strong><font color="blue">Chuanguang Yang</font></strong>, Zhulin An, Linhang Cai, Yongjun Xu<br>
                in <i>AAAI Conference on Artificial Intelligence</i> (AAAI-2022 Oral)<br>
                <alert>CCF-A; Acceptance Rate: 1349/9020=15.0%; Oral Rate: Top 5% </alert>
                <br>
                [<a href="https://www.aaai.org/AAAI22Papers/AAAI-1139.YangC.pdf">Paper</a>]
                [<a href="https://github.com/winycg/MCL">Code</a>]
                [<a href="./resources/citation/AAAI_2022_MCL.html">BibTex</a>]
                <br>
                We propose a simple yet effective mutual contrastive learning approach to learn better feature representations for both supervised and self-supervised image classification.
             
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./resources/paper_icon/AAAI_2022_PGMPF.png"
                                title="Prior Gradient Mask Guided Pruning-aware Fine-tuning">
            <div><strong>Prior Gradient Mask Guided Pruning-aware Fine-tuning</strong><br>
                Linhang Cai, Zhulin An, <strong><font color="blue">Chuanguang Yang</font></strong>, Yanchun Yang, Yongjun Xu<br>
                in <i>AAAI Conference on Artificial Intelligence</i> (AAAI-2022)<br>
                <alert>CCF-A; Acceptance Rate: 1349/9020=15.0%</alert>
                <br>
                [<a href="https://www.aaai.org/AAAI22Papers/AAAI-1708.CaiL.pdf">Paper</a>]
                [<a href="https://github.com/cailinhang/PGMPF">Code</a>]
                [<a href="./resources/citation/AAAI_2022_PGMGD.html">BibTex</a>]
                <br>
                We proposed a prior gradient mask guided pruning-aware fine-tuning framework to accelerate CNNs for image classification.
           
            </div>
            <div class="spanner"></div>
        </div>

        <h3 id="confpapers">2021</h3>

        <div class="paper"><img class="paper" src="./resources/paper_icon/IJCAI_2021_HSAKD.png"
                                title="Hierarchical Self-supervised Augmented Knowledge Distillation">
            <div><strong>Hierarchical Self-supervised Augmented Knowledge Distillation</strong><br>
                <strong><font color="blue">Chuanguang Yang</font></strong>, Zhulin An, Linhang Cai, Yongjun Xu<br>
                in <i>International Joint Conference on Artificial Intelligence</i> (IJCAI-2021)<br>
                <alert>CCF-A; Acceptance Rate: 587/4204=13.9%</alert>
                <br>
                [<a href="https://www.ijcai.org/proceedings/2021/0168.pdf">Paper</a>]
                [<a href="https://github.com/winycg/HSAKD">Code</a>]
                [<a href="./resources/citation/IJCAI_2021_HSAKD.html">BibTex</a>]
                <br>
                <br>
                 We propose a strong self-supervised augmented knowledge distillation method from hierarchical feature maps for image classification.
                
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./resources/paper_icon/ICASSP_2021_MVCL.png"
                                title="Multi-View Contrastive Learning for Online Knowledge Distillation">
            <div><strong>Multi-View Contrastive Learning for Online Knowledge Distillation</strong><br>
                <strong><font color="blue">Chuanguang Yang</font></strong>, Zhulin An, Yongjun Xu<br>
                in <i>IEEE International Conference on Acoustics, Speech and Signal Processing</i> (ICASSP-2021)<br>
                <alert>CCF-B; Acceptance Rate: 1734/3610=48.0%</alert><br>
                [<a href="https://arxiv.org/pdf/2006.04093.pdf">Paper</a>]
                [<a href="https://github.com/winycg/MCL-OKD">Code</a>]
                [<a href="./resources/citation/ICASSP_2021_MCL_OKD.html">BibTex</a>]
                <br>
                <br>
                 We propose multi-view contrastive learning to perform online knowledge distillation for image classification.
            </div>
            <div class="spanner"></div>
        </div>
        <h3 id="confpapers">2020</h3>
        <div class="paper"><img class="paper" src="./resources/paper_icon/ECAI_2020_DRNet.png"
                                title="DRNet: Dissect and Reconstruct the Convolutional Neural Network via Interpretable Manners">
            <div><strong>DRNet: Dissect and Reconstruct the Convolutional Neural Network via Interpretable Manners</strong><br>
                Xiaolong Hu, Zhulin An, <strong><font color="blue">Chuanguang Yang</font></strong>, Hui Zhu, Kaiqaing Xu, Yongjun Xu<br>
                in <i>European Conference on Artificial Intelligence</i> (ECAI-2020)<br>
                <alert>CCF-B; Acceptance Rate: 365/1363=26.8%</alert><br>
                [<a href="https://ecai2020.eu/papers/447_paper.pdf">Paper</a>]
                [<a href="./resources/citation/ECAI_2020_DRNet.html">BibTex</a>]
                <br>
                <br>
                We propose an interpretable manner to dynamically run channels for sub-class image classification.
                
            </div>
            <div class="spanner"></div>
        </div>


        <div class="paper"><img class="paper" src="./resources/paper_icon/AAAI_2020_HCGNet.png"
                                title="Gated Convolutional Networks with Hybrid Connectivity for Image Classification
">
            <div><strong>Gated Convolutional Networks with Hybrid Connectivity for Image Classification
</strong><br>
                <strong><font color="blue">Chuanguang Yang</font></strong>, Zhulin An, Hui Zhu, Xiaolong Hu, Kun Zhang, Kaiqiang Xu, Chao Li, Yongjun Xu<br>
                in <i>AAAI Conference on Artificial Intelligence</i> (AAAI-2020)<br>
                <alert>CCF-A; Acceptance Rate: 1591/7737=20.6%</alert><br>
                [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/6948/6802">Paper</a>]
                [<a href="https://github.com/winycg/HCGNet">Code</a>]
                [<a href="./resources/citation/AAAI_2020_HCGNet.html">BibTex</a>]
                
                <br>
                <br>
                We propose a new network architecture called HCGNet equipped with novel hybrid connectivity and gated mechanisms for image classification.
                
            </div>
            <div class="spanner"></div>
        </div>
        <h3 id="confpapers">2019</h3>
<div class="paper"><img class="paper" src="./resources/paper_icon/ICCVW_2021_EENA.png"
                                title="EENA: Efficient Evolution of Neural Architecture
">
            <div><strong>EENA: Efficient Evolution of Neural Architecture</strong><br>
                Hui Zhu, Zhulin An, <strong><font color="blue">Chuanguang Yang</font></strong>, Kaiqiang Xu, Erhu Zhao, Yongjun Xu<br>
                in <i>International Conference on Computer Vision Workshops</i> (ICCVW-2019)<br>
                <alert>CCF-A Workshop</alert><br>
                [<a href="https://openaccess.thecvf.com/content_ICCVW_2019/papers/NeurArch/Zhu_EENA_Efficient_Evolution_of_Neural_Architecture_ICCVW_2019_paper.pdf">Paper</a>]
                [<a href="./resources/citation/ICCVW_2019_EENA.html">BibTex</a>]
                <br>
                <br>
                 We propose an efficient evolution algorithm for neural architecture search for image classification.
                
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./resources/paper_icon/ICANN_2019_GA.png"
            title="Multi-objective Pruning for CNNs using Genetic Algorithm
        ">
        <div><strong>Multi-objective Pruning for CNNs using Genetic Algorithm
        </strong><br>
        <strong><font color="blue">Chuanguang Yang</font></strong>, Zhulin An, Chao Li, Boyu Diao, Yongjun Xu<br>
        in <i>International Conference on Artificial Neural Networks</i> (ICANN-2019)<br>
        <alert>CCF-C</alert><br>
        [<a href="https://arxiv.org/pdf/1906.00399.pdf">Paper</a>]
        [<a href="./resources/citation/ICANN_2019_MOP.html">BibTex</a>]
        <br>
        <br>
        We propose a genetic algorithm to deal with model pruning using a multi-objective fitness function.

        </div>
        <div class="spanner"></div>
        </div>





</body>
</html>
