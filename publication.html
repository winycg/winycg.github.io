<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Chuanguang Yang</title>
    <meta content="Chuanguang Yang, https://winycg.github.io" name="keywords">
    <style media="screen" type="text/css">
        html,
        body,
        div,
        span,
        applet,
        object,
        iframe,
        h1,
        h2,
        h3,
        h4,
        h5,
        h6,
        p,
        blockquote,
        pre,
        a,
        abbr,
        acronym,
        address,
        big,
        cite,
        code,
        del,
        dfn,
        em,
        font,
        img,
        ins,
        kbd,
        q,
        s,
        samp,
        small,
        strike,
        strong,
        sub,
        tt,
        var,
        dl,
        dt,
        dd,
        ol,
        ul,
        li,
        fieldset,
        form,
        label,
        legend,
        table,
        caption,
        tbody,
        tfoot,
        thead,
        tr,
        th,
        td {
            border: 0pt none;
            font-family: Times New Roman;
            font-size: 100%;
            font-style: inherit;
            font-weight: inherit;
            margin: 0pt;
            outline-color: invert;
            outline-style: none;
            outline-width: 0pt;
            padding: 0pt;
            vertical-align: baseline;
        }

        a {
            color: #1772d0;
            text-decoration: none;
        }

        a:focus,
        a:hover {
            color: #f09228;
            text-decoration: none;
        }

        a.paper {
            font-weight: bold;
            font-size: 12pt;
        }

        b.paper {
            font-weight: bold;
            font-size: 12pt;
        }

        * {
            margin: 0pt;
            padding: 0pt;
        }

        body {
            position: relative;
            margin: 3em auto 2em auto;
            width: 800px;
            font-family: Times New Roman;
            font-size: 18px;
            background: #eee;
        }

        h2 {
            font-family: Times New Roman;
            font-size: 18pt;
            font-weight: 700;
        }

        h3 {
            font-family: Times New Roman;
            font-size: 18px;
            font-weight: 700;
        }

        strong {
            font-family: Times New Roman;
            font-size: 18px;
            font-weight: bold;
        }

        ul {
            list-style: circle;
        }

        img {
            border: none;
        }

        li {
            padding-bottom: 0.5em;
            margin-left: 1.4em;
        }

        alert {
            font-family: Times New Roman;
            font-size: 18px;
            font-weight: bold;
            color: #FF0000;
        }

        em,
        i {
            font-style: italic;
        }

        div.section {
            clear: both;
            margin-bottom: 1.5em;
            background: #eee;
        }

        div.spanner {
            clear: both;
        }

        div.paper {
            clear: both;
            margin-top: 0.5em;
            margin-bottom: 1em;
            border: 1px solid #ddd;
            background: #fff;
            padding: 1em 1em 1em 1em;
        }

        div.paper div {
            padding-left: 230px;
        }

        img.paper {
            margin-bottom: 0.5em;
            float: left;
            width: 200px;
        }

        span.blurb {
            font-style: italic;
            display: block;
            margin-top: 0.75em;
            margin-bottom: 0.5em;
        }

        pre,
        code {
            font-family: Times New Roman;
            margin: 1em 0;
            padding: 0;
        }

        div.paper pre {
            font-size: 0.9em;
        }
    </style>

    <link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet"
          type="text/css"/>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-164510176-1"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-164510176-1');
    </script>

</head>


<body>
<!--<div style="clear: both; background-color: #fff; margin-top: 1.5em; padding: .2em; padding-left: .3em;">-->





<div style="clear: both;">
    <div class="section">
        <br>
        <h2 id="confpapers">Selected Publications</h2>
        <h3 id="confpapers">2024</h3>
        
        <div class="paper"><img class="paper" src="./resources/paper_icon/ICML_2024_DetKDS.jpg"
            title="DetKDS: Knowledge Distillation Search for Object Detectors">
        <div><strong>DetKDS: Knowledge Distillation Search for Object Detectors.</strong><br>
            Lujun Li, Yufan Bao, Peijie Dong, <strong><font color="blue">Chuanguang Yang</font></strong>, Anggeng Li, Wenhan Luo, Qifeng Liu, Wei Xue, Yike Guo.
        <br>
        in <i>International Conference on Machine Learning</i> (ICML-2024)<br>
        <alert>CCF-A, Acceptance rate: 2610/9473=27.5%</alert>
        <br>
                [<a href="https://openreview.net/pdf?id=SBR8Gwe1E2">Paper</a>]
                [<a href="https://github.com/lliai/DetKDS">Code</a>]
                <br>
                We leverage search algorithms to discover optimal distillers for object detectors.
        </div>
        <div class="spanner"></div>
        </div>


        <div class="paper"><img class="paper" src="./resources/paper_icon/IJCNN_2024_OPD.jpg"
            title="Online Policy Distillation with Decision-Attention">
        <div><strong>Online Policy Distillation with Decision-Attention.</strong><br>
            Xinqiang Yu, <strong><font color="blue">Chuanguang Yang</font></strong>, Chengqing Yu, Libo Huang, Zhulin An, Yongjun Xu.
        <br>
        in <i>International Joint Conference on Neural Networks</i> (IJCNN-2024)<br>
        <alert>CCF-C, Acceptance rate: 1701/3272=51.99%</alert>
        <br>
                [<a href="https://arxiv.org/pdf/2406.05488">Paper</a>]
                <br>
                We propose Online Policy Distillation with Decision-Attention to make various policies operate in the same environment learn different perspectives.
        </div>
        <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./resources/paper_icon/CVPR_2024_CLIP_KD.jpg"
            title="CLIP-KD: An Empirical Study of CLIP Model Distillation">
        <div><strong>CLIP-KD: An Empirical Study of CLIP Model Distillation.</strong><br>
            <strong><font color="blue">Chuanguang Yang</font></strong>, Zhulin An, Libo Huang, Junyu Bi, Xinqiang Yu, Han Yang, Boyu Diao, Yongjun Xu.
        <br>
        in <i>IEEE/CVF Conference on Computer Vision and Pattern Recognition</i> (CVPR-2024)<br>
        <alert>CCF-A, Acceptance rate: 2719/11532=23.6%</alert>
        <br>
                [<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Yang_CLIP-KD_An_Empirical_Study_of_CLIP_Model_Distillation_CVPR_2024_paper.pdf">Paper</a>]
                [<a href="https://github.com/winycg/CLIP-KD">Code</a>]
                <br>
                We propose several distillation strategies, including relation, feature, gradient and contrastive paradigms, to examine the effectiveness of CLIP-Knowledge Distillation.
        </div>
        <div class="spanner"></div>
        </div>
        
        <div class="paper"><img class="paper" src="./resources/paper_icon/AAAI_2024_eTag.jpg"
            title="eTag: Class-Incremental Learning with Embedding Distillation and Task-Oriented Generation.">
        <div><strong>eTag: Class-Incremental Learning with Embedding Distillation and Task-Oriented Generation.</strong><br>
        Libo Huang, Yan Zeng, <strong><font color="blue">Chuanguang Yang</font></strong>, Zhulin An, Boyu Diao, Yongjun Xu.<br>
        in <i>AAAI Conference on Artificial Intelligence</i> (AAAI-2024)<br>
        <alert>CCF-A, Acceptance rate: 2342/12100=23.8%</alert>
        <br>
                [<a href="https://arxiv.org/pdf/2304.10103.pdf">Paper</a>]
                <br>
                We propose a method of embedding distillation and task-oriented generation for class-incremental learning, which requires
neither the exemplar nor the prototype.
        </div>
        <div class="spanner"></div>
        </div>

        <h3 id="confpapers">2023</h3>
        <div class="paper"><img class="paper" src="./resources/paper_icon/ICCV_2023_VL_MATCH.jpg"
            title="VL-Match: Enhancing Vision-Language Pretraining with Token-Level and Instance-Level Matching">
        <div><strong>VL-Match: Enhancing Vision-Language Pretraining with Token-Level and Instance-Level Matching.</strong><br>
        Junyu Bi, Daixuan Cheng, Ping Yao, Bochen Pang, Yuefeng Zhan, <strong><font color="blue">Chuanguang Yang</font></strong>, et al.<br>
        in <i>IEEE/CVF International Conference on Computer Vision</i> (ICCV-2023)<br>
        <alert>CCF-A, Acceptance rate: 2160/8068=26.8%</alert>
        <br>
                [<a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Bi_VL-Match_Enhancing_Vision-Language_Pretraining_with_Token-Level_and_Instance-Level_Matching_ICCV_2023_paper.pdf">Paper</a>]
                <br>
                We
propose VL-Match, a Vision-Language framework with Enhanced Token-level and Instance-level Matching. 
        </div>
        <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./resources/paper_icon/Springer_2023_survey.jpg"
            title="Categories of Response-Based, Feature-Based, and Relation-Based Knowledge Distillation">
        <div><strong>Categories of Response-Based, Feature-Based, and Relation-Based Knowledge Distillation.</strong><br>
        <strong><font color="blue">Chuanguang Yang</font></strong>, Xinqiang Yu, Zhulin An, Yongjun Xu.<br>
        in <i>Advancements in Knowledge Distillation: Towards New Horizons of Intelligent Systems</i> (Springer Book Chapter)<br>
        <alert>Invited Survey Paper</alert>
        <br>
                [<a href="https://arxiv.org/pdf/2306.10687.pdf">Paper</a>]
                <br>
                This paper provides a comprehensive KD survey,
including knowledge categories, distillation schemes and algorithms, as
well as some empirical studies on performance comparison. 
        </div>
        <div class="spanner"></div>
        </div>


        <div class="paper"><img class="paper" src="./resources/paper_icon/TPAMI_2023_L_MCL.jpg"
            title="Online Knowledge Distillation via Mutual Contrastive Learning for Visual Recognition.">
        <div><strong>Online Knowledge Distillation via Mutual Contrastive Learning for Visual Recognition.</strong><br>
        <strong><font color="blue">Chuanguang Yang</font></strong>, Zhulin An, Helong Zhou, Fuzhen Zhuang, Yongjun Xu, Qian Zhang.<br>
        in <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i> (TPAMI-2023)<br>
        <alert>CCF-A</alert>
        <br>
                [<a href="https://arxiv.org/pdf/2207.11518.pdf">Paper</a>]
                [<a href="https://github.com/winycg/L-MCL">Code</a>]
                <br>
                We present a Mutual Contrastive Learning (MCL) framework for online KD. The core idea of MCL is to perform mutual interaction and transfer of contrastive distributions
among a cohort of networks in an online manner. 
        </div>
        <div class="spanner"></div>
        </div>

        <h3 id="confpapers">2022</h3>
        <div class="paper"><img class="paper" src="./resources/paper_icon/ECCV_2022_MixSKD.jpg"
            title="MixSKD: Self-Knowledge Distillation from Mixup for Image Recognition.">
        <div><strong>MixSKD: Self-Knowledge Distillation from Mixup for Image Recognition.</strong><br>
        <strong><font color="blue">Chuanguang Yang</font></strong>, Zhulin An, Helong Zhou, Linhang Cai, Xiang Zhi, Jiwen Wu, Yongjun Xu, Qian Zhang.<br>
        in <i>European Conference on Computer Vision</i> (ECCV-2022)<br>
        <alert>CCF-B, Acceptance rate: 1650/5803=28.4%</alert>
        <br>
                [<a href="https://arxiv.org/pdf/2208.05768.pdf">Paper</a>]
                [<a href="https://github.com/winycg/Self-KD-Lib">Code</a>]
                <br>
                This paper presents MixSKD, a powerful Self-KD method to regularize the network to behave linearly in feature maps and class probabilities between samples
                using Mixup images.
        </div>
        <div class="spanner"></div>
        </div>


        <div class="paper"><img class="paper" src="./resources/paper_icon/TNNLS_HSSAKD.png"
            title="Knowledge Distillation Using Hierarchical Self-Supervision Augmented Distribution">
        <div><strong>Knowledge Distillation Using Hierarchical Self-Supervision Augmented Distribution</strong><br>
        <strong><font color="blue">Chuanguang Yang</font></strong>, Zhulin An, Linhang Cai, Yongjun Xu<br>
        in <i>IEEE Transactions on Neural Networks and Learning Systems</i> (TNNLS-2022)<br>
        <alert>CCF-B, IF:14.255</alert>
        <br>
                [<a href="https://arxiv.org/pdf/2109.03075.pdf">Paper</a>]
                [<a href="https://github.com/winycg/HSAKD">Code</a>]
                <br>
        We investigate an auxiliary self-supervision augmented distributionan for training a single network, offline KD and online KD.
        </div>
        <div class="spanner"></div>
        </div>


        <div class="paper"><img class="paper" src="./resources/paper_icon/ICME_2022_anchornet.png"
            title="Localizing Semantic Patches for Accelerating Image Classification">
        <div><strong>Localizing Semantic Patches for Accelerating Image Classification</strong><br>
        <strong><font color="blue">Chuanguang Yang</font></strong>, Zhulin An, Yongjun Xu<br>
        in <i>IEEE International Conference on Multimedia and Expo</i> (ICME-2022)<br>
        <alert>CCF-B, Acceptance rate: 381/1285=29.6%</alert>
        <br>
                [<a href="https://arxiv.org/pdf/2206.03367.pdf">Paper</a>]
                [<a href="https://github.com/winycg/AnchorNet">Code</a>]
                <br>
        We propose an interpretable AnchorNet to localize semantic patches for accelerating image classification.
        </div>
        <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./resources/paper_icon/CVPR_2022_CIRKD.png"
            title="Cross-Image Relational Knowledge Distillation for Semantic Segmentation.">
        <div><strong>Cross-Image Relational Knowledge Distillation for Semantic Segmentation</strong><br>
            <strong><font color="blue">Chuanguang Yang</font></strong>, Helong Zhou, Zhulin An, Xue Jiang, Yongjun Xu, Qian Zhang<br>
        in <i>IEEE/CVF Conference on Computer Vision and Pattern Recognition </i> (CVPR-2022)<br>
        <alert>CCF-A, Acceptance rate: 2067/8161=25.3%</alert>
        <br>
                [<a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Yang_Cross-Image_Relational_Knowledge_Distillation_for_Semantic_Segmentation_CVPR_2022_paper.pdf">Paper</a>]
                [<a href="https://github.com/winycg/CIRKD">Code</a>]
                <br>
         We propose cross-Image relational knowledge distillation for semantic segmentation.
        
        </div>
        <div class="spanner"></div>
        </div>


        <div class="paper"><img class="paper" src="./resources/paper_icon/AAAI_2022_MCL.png"
                                title="Mutual Contrastive Learning for Visual Representation Learning">
            <div><strong>Mutual Contrastive Learning for Visual Representation Learning</strong><br>
                <strong><font color="blue">Chuanguang Yang</font></strong>, Zhulin An, Linhang Cai, Yongjun Xu<br>
                in <i>AAAI Conference on Artificial Intelligence</i> (AAAI-2022 Oral)<br>
                <alert>CCF-A; Acceptance Rate: 1349/9020=15.0%; Oral Rate: Top 5% </alert>
                <br>
                [<a href="https://www.aaai.org/AAAI22Papers/AAAI-1139.YangC.pdf">Paper</a>]
                [<a href="https://github.com/winycg/MCL">Code</a>]
                <br>
                We propose a simple yet effective mutual contrastive learning approach to learn better feature representations for both supervised and self-supervised image classification.
             
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./resources/paper_icon/AAAI_2022_PGMPF.png"
                                title="Prior Gradient Mask Guided Pruning-aware Fine-tuning">
            <div><strong>Prior Gradient Mask Guided Pruning-aware Fine-tuning</strong><br>
                Linhang Cai, Zhulin An, <strong><font color="blue">Chuanguang Yang</font></strong>, Yanchun Yang, Yongjun Xu<br>
                in <i>AAAI Conference on Artificial Intelligence</i> (AAAI-2022)<br>
                <alert>CCF-A; Acceptance Rate: 1349/9020=15.0%</alert>
                <br>
                [<a href="https://www.aaai.org/AAAI22Papers/AAAI-1708.CaiL.pdf">Paper</a>]
                [<a href="https://github.com/cailinhang/PGMPF">Code</a>]
                <br>
                We proposed a prior gradient mask guided pruning-aware fine-tuning framework to accelerate CNNs for image classification.
           
            </div>
            <div class="spanner"></div>
        </div>

        <h3 id="confpapers">2021</h3>

        <div class="paper"><img class="paper" src="./resources/paper_icon/IJCAI_2021_HSAKD.png"
                                title="Hierarchical Self-supervised Augmented Knowledge Distillation">
            <div><strong>Hierarchical Self-supervised Augmented Knowledge Distillation</strong><br>
                <strong><font color="blue">Chuanguang Yang</font></strong>, Zhulin An, Linhang Cai, Yongjun Xu<br>
                in <i>International Joint Conference on Artificial Intelligence</i> (IJCAI-2021)<br>
                <alert>CCF-A; Acceptance Rate: 587/4204=13.9%</alert>
                <br>
                [<a href="https://www.ijcai.org/proceedings/2021/0168.pdf">Paper</a>]
                [<a href="https://github.com/winycg/HSAKD">Code</a>]
                <br>
                <br>
                 We propose a strong self-supervised augmented knowledge distillation method from hierarchical feature maps for image classification.
                
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./resources/paper_icon/ICASSP_2021_MVCL.png"
                                title="Multi-View Contrastive Learning for Online Knowledge Distillation">
            <div><strong>Multi-View Contrastive Learning for Online Knowledge Distillation</strong><br>
                <strong><font color="blue">Chuanguang Yang</font></strong>, Zhulin An, Yongjun Xu<br>
                in <i>IEEE International Conference on Acoustics, Speech and Signal Processing</i> (ICASSP-2021)<br>
                <alert>CCF-B; Acceptance Rate: 1734/3610=48.0%</alert><br>
                [<a href="https://arxiv.org/pdf/2006.04093.pdf">Paper</a>]
                [<a href="https://github.com/winycg/MCL-OKD">Code</a>]
                <br>
                <br>
                 We propose multi-view contrastive learning to perform online knowledge distillation for image classification.
            </div>
            <div class="spanner"></div>
        </div>
        <h3 id="confpapers">2020</h3>
        <div class="paper"><img class="paper" src="./resources/paper_icon/ECAI_2020_DRNet.png"
                                title="DRNet: Dissect and Reconstruct the Convolutional Neural Network via Interpretable Manners">
            <div><strong>DRNet: Dissect and Reconstruct the Convolutional Neural Network via Interpretable Manners</strong><br>
                Xiaolong Hu, Zhulin An, <strong><font color="blue">Chuanguang Yang</font></strong>, Hui Zhu, Kaiqaing Xu, Yongjun Xu<br>
                in <i>European Conference on Artificial Intelligence</i> (ECAI-2020)<br>
                <alert>CCF-B; Acceptance Rate: 365/1363=26.8%</alert><br>
                [<a href="https://ecai2020.eu/papers/447_paper.pdf">Paper</a>]
                <br>
                <br>
                We propose an interpretable manner to dynamically run channels for sub-class image classification.
                
            </div>
            <div class="spanner"></div>
        </div>


        <div class="paper"><img class="paper" src="./resources/paper_icon/AAAI_2020_HCGNet.png"
                                title="Gated Convolutional Networks with Hybrid Connectivity for Image Classification
">
            <div><strong>Gated Convolutional Networks with Hybrid Connectivity for Image Classification
</strong><br>
                <strong><font color="blue">Chuanguang Yang</font></strong>, Zhulin An, Hui Zhu, Xiaolong Hu, Kun Zhang, Kaiqiang Xu, Chao Li, Yongjun Xu<br>
                in <i>AAAI Conference on Artificial Intelligence</i> (AAAI-2020)<br>
                <alert>CCF-A; Acceptance Rate: 1591/7737=20.6%</alert><br>
                [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/6948/6802">Paper</a>]
                [<a href="https://github.com/winycg/HCGNet">Code</a>]
                
                <br>
                <br>
                We propose a new network architecture called HCGNet equipped with novel hybrid connectivity and gated mechanisms for image classification.
                
            </div>
            <div class="spanner"></div>
        </div>
        <h3 id="confpapers">2019</h3>
<div class="paper"><img class="paper" src="./resources/paper_icon/ICCVW_2021_EENA.png"
                                title="EENA: Efficient Evolution of Neural Architecture
">
            <div><strong>EENA: Efficient Evolution of Neural Architecture</strong><br>
                Hui Zhu, Zhulin An, <strong><font color="blue">Chuanguang Yang</font></strong>, Kaiqiang Xu, Erhu Zhao, Yongjun Xu<br>
                in <i>International Conference on Computer Vision Workshops</i> (ICCVW-2019)<br>
                <alert>CCF-A Workshop</alert><br>
                [<a href="https://openaccess.thecvf.com/content_ICCVW_2019/papers/NeurArch/Zhu_EENA_Efficient_Evolution_of_Neural_Architecture_ICCVW_2019_paper.pdf">Paper</a>]
                <br>
                <br>
                 We propose an efficient evolution algorithm for neural architecture search for image classification.
                
            </div>
            <div class="spanner"></div>
        </div>

        <div class="paper"><img class="paper" src="./resources/paper_icon/ICANN_2019_GA.png"
            title="Multi-objective Pruning for CNNs using Genetic Algorithm
        ">
        <div><strong>Multi-objective Pruning for CNNs using Genetic Algorithm
        </strong><br>
        <strong><font color="blue">Chuanguang Yang</font></strong>, Zhulin An, Chao Li, Boyu Diao, Yongjun Xu<br>
        in <i>International Conference on Artificial Neural Networks</i> (ICANN-2019)<br>
        <alert>CCF-C</alert><br>
        [<a href="https://arxiv.org/pdf/1906.00399.pdf">Paper</a>]
        <br>
        <br>
        We propose a genetic algorithm to deal with model pruning using a multi-objective fitness function.

        </div>
        <div class="spanner"></div>
        </div>





</body>
</html>
